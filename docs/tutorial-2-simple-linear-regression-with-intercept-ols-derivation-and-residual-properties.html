<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>3 Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties | Introduction to Econometrics</title>
<meta name="author" content="Nicolás Campos Bijit">
<meta name="description" content="3.1 Setup We study the simple linear regression with an intercept: \[ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,\qquad i=1,\ldots,n, \] with the usual regularity condition \(\sum_{i=1}^n...">
<meta name="generator" content="bookdown 0.46 with bs4_book()">
<meta property="og:title" content="3 Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties | Introduction to Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Setup We study the simple linear regression with an intercept: \[ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,\qquad i=1,\ldots,n, \] with the usual regularity condition \(\sum_{i=1}^n...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3 Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties | Introduction to Econometrics">
<meta name="twitter:description" content="3.1 Setup We study the simple linear regression with an intercept: \[ Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,\qquad i=1,\ldots,n, \] with the usual regularity condition \(\sum_{i=1}^n...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Source_Sans_3-0.4.10/font.css" rel="stylesheet">
<link href="libs/IBM_Plex_Mono-0.4.10/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style4.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Introduction to Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Introducción</a></li>
<li><a class="" href="tutorial-1-data-analysis-with-r.html"><span class="header-section-number">2</span> Tutorial 1: Data Analysis with R</a></li>
<li><a class="active" href="tutorial-2-simple-linear-regression-with-intercept-ols-derivation-and-residual-properties.html"><span class="header-section-number">3</span> Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="tutorial-2-simple-linear-regression-with-intercept-ols-derivation-and-residual-properties" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties<a class="anchor" aria-label="anchor" href="#tutorial-2-simple-linear-regression-with-intercept-ols-derivation-and-residual-properties"><i class="fas fa-link"></i></a>
</h1>
<div id="setup" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Setup<a class="anchor" aria-label="anchor" href="#setup"><i class="fas fa-link"></i></a>
</h2>
<p>We study the <strong>simple linear regression with an intercept</strong>:
<span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,\qquad i=1,\ldots,n,
\]</span>
with the usual regularity condition <span class="math inline">\(\sum_{i=1}^n (X_i-\bar X)^2&gt;0\)</span> (i.e., not all <span class="math inline">\(X_i\)</span> are equal).</p>
<p>Define sample means:
<span class="math display">\[
\bar Y = \frac{1}{n}\sum_{i=1}^n Y_i,
\qquad
\bar X = \frac{1}{n}\sum_{i=1}^n X_i.
\]</span></p>
<p>OLS chooses <span class="math inline">\((\hat\beta_0,\hat\beta_1)\)</span> to minimize the <strong>sum of squared residuals</strong>:
<span class="math display">\[
S(\beta_0,\beta_1)
=
\sum_{i=1}^n \left(Y_i-\beta_0-\beta_1 X_i\right)^2.
\]</span></p>
<p>Throughout, define <strong>fitted values</strong> and <strong>residuals</strong>:
<span class="math display">\[
\hat Y_i = \hat\beta_0 + \hat\beta_1 X_i,
\qquad
\hat u_i = Y_i - \hat Y_i.
\]</span></p>
<hr>
</div>
<div id="step-1-derive-the-ols-normal-equations" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Step 1: Derive the OLS normal equations<a class="anchor" aria-label="anchor" href="#step-1-derive-the-ols-normal-equations"><i class="fas fa-link"></i></a>
</h2>
<p>Differentiate <span class="math inline">\(S(\beta_0,\beta_1)\)</span> with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<div id="foc-for-beta_0" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> FOC for <span class="math inline">\(\beta_0\)</span><a class="anchor" aria-label="anchor" href="#foc-for-beta_0"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[
\frac{\partial S}{\partial \beta_0}
=
\sum_{i=1}^n 2\left(Y_i-\beta_0-\beta_1X_i\right)(-1)=0
\]</span>
<span class="math display">\[
\Longrightarrow\quad
\sum_{i=1}^n \left(Y_i-\beta_0-\beta_1X_i\right)=0.
\]</span></p>
<p>This implies
<span class="math display">\[
\sum_{i=1}^n Y_i
=
n\beta_0+\beta_1\sum_{i=1}^n X_i
\quad\Longrightarrow\quad
\hat\beta_0
=
\bar Y-\hat\beta_1\bar X.
\]</span></p>
</div>
<div id="foc-for-beta_1" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> FOC for <span class="math inline">\(\beta_1\)</span><a class="anchor" aria-label="anchor" href="#foc-for-beta_1"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[
\frac{\partial S}{\partial \beta_1}
=
\sum_{i=1}^n 2\left(Y_i-\beta_0-\beta_1X_i\right)(-X_i)=0
\]</span>
<span class="math display">\[
\Longrightarrow\quad
\sum_{i=1}^n X_i\left(Y_i-\beta_0-\beta_1X_i\right)=0.
\]</span></p>
<hr>
</div>
</div>
<div id="step-2-derive-the-slope-in-centered-form" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Step 2: Derive the slope in centered form<a class="anchor" aria-label="anchor" href="#step-2-derive-the-slope-in-centered-form"><i class="fas fa-link"></i></a>
</h2>
<p>Plug <span class="math inline">\(\beta_0=\bar Y-\beta_1\bar X\)</span> into the residual expression:
<span class="math display">\[
Y_i-\beta_0-\beta_1X_i
=
Y_i-(\bar Y-\beta_1\bar X)-\beta_1X_i
=
(Y_i-\bar Y)-\beta_1(X_i-\bar X).
\]</span></p>
<p>Then the <span class="math inline">\(\beta_1\)</span> FOC becomes:
<span class="math display">\[
\sum_{i=1}^n X_i\Big((Y_i-\bar Y)-\beta_1(X_i-\bar X)\Big)=0.
\]</span></p>
<p>A cleaner equivalent (and standard) route is to multiply by <span class="math inline">\((X_i-\bar X)\)</span> instead of <span class="math inline">\(X_i\)</span>. Using the expression above, minimizing
<span class="math inline">\(\sum_i\big((Y_i-\bar Y)-\beta_1(X_i-\bar X)\big)^2\)</span>
in <span class="math inline">\(\beta_1\)</span> yields the FOC:
<span class="math display">\[
\sum_{i=1}^n (X_i-\bar X)\Big((Y_i-\bar Y)-\beta_1(X_i-\bar X)\Big)=0,
\]</span>
so
<span class="math display">\[
\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y)
=
\beta_1\sum_{i=1}^n (X_i-\bar X)^2.
\]</span></p>
<p>Therefore,
<span class="math display">\[
\boxed{
\hat\beta_1
=
\frac{\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y)}{\sum_{i=1}^n (X_i-\bar X)^2}
}
\qquad\text{and}\qquad
\boxed{
\hat\beta_0
=
\bar Y-\hat\beta_1\bar X.
}
\]</span></p>
<hr>
</div>
<div id="step-3-residual-orthogonality-core-properties" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Step 3: Residual orthogonality (core properties)<a class="anchor" aria-label="anchor" href="#step-3-residual-orthogonality-core-properties"><i class="fas fa-link"></i></a>
</h2>
<p>These are the key “extra” results you can add after the derivation, mirroring the structure of the no-intercept exercise.</p>
<div id="residuals-sum-to-zero" class="section level3" number="3.4.1">
<h3>
<span class="header-section-number">3.4.1</span> 3.1 Residuals sum to zero<a class="anchor" aria-label="anchor" href="#residuals-sum-to-zero"><i class="fas fa-link"></i></a>
</h3>
<p>From the <span class="math inline">\(\beta_0\)</span> normal equation evaluated at <span class="math inline">\((\hat\beta_0,\hat\beta_1)\)</span>:
<span class="math display">\[
\sum_{i=1}^n \left(Y_i-\hat\beta_0-\hat\beta_1X_i\right)=0
\quad\Longrightarrow\quad
\boxed{\sum_{i=1}^n \hat u_i = 0.}
\]</span></p>
</div>
<div id="residuals-are-orthogonal-to-x" class="section level3" number="3.4.2">
<h3>
<span class="header-section-number">3.4.2</span> 3.2 Residuals are orthogonal to <span class="math inline">\(X\)</span><a class="anchor" aria-label="anchor" href="#residuals-are-orthogonal-to-x"><i class="fas fa-link"></i></a>
</h3>
<p>From the <span class="math inline">\(\beta_1\)</span> normal equation at the OLS solution:
<span class="math display">\[
\sum_{i=1}^n X_i\left(Y_i-\hat\beta_0-\hat\beta_1X_i\right)=0
\quad\Longrightarrow\quad
\boxed{\sum_{i=1}^n X_i\,\hat u_i=0.}
\]</span></p>
</div>
<div id="residuals-are-orthogonal-to-centered-regressor-x_i-bar-x" class="section level3" number="3.4.3">
<h3>
<span class="header-section-number">3.4.3</span> 3.3 Residuals are orthogonal to centered regressor <span class="math inline">\((X_i-\bar X)\)</span><a class="anchor" aria-label="anchor" href="#residuals-are-orthogonal-to-centered-regressor-x_i-bar-x"><i class="fas fa-link"></i></a>
</h3>
<p>Use <span class="math inline">\(\sum_i \hat u_i=0\)</span> to write:
<span class="math display">\[
\sum_{i=1}^n (X_i-\bar X)\hat u_i
=
\sum_{i=1}^n X_i\hat u_i
-
\bar X\sum_{i=1}^n \hat u_i
=
0-\bar X\cdot 0
=0.
\]</span>
So,
<span class="math display">\[
\boxed{\sum_{i=1}^n (X_i-\bar X)\hat u_i=0.}
\]</span></p>
<hr>
</div>
</div>
<div id="step-4-more-implications-involving-fitted-values-and-means" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Step 4: More implications involving fitted values and means<a class="anchor" aria-label="anchor" href="#step-4-more-implications-involving-fitted-values-and-means"><i class="fas fa-link"></i></a>
</h2>
<div id="the-regression-line-goes-through-bar-xbar-y" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> 4.1 The regression line goes through <span class="math inline">\((\bar X,\bar Y)\)</span><a class="anchor" aria-label="anchor" href="#the-regression-line-goes-through-bar-xbar-y"><i class="fas fa-link"></i></a>
</h3>
<p>Take averages of fitted values:
<span class="math display">\[
\overline{\hat Y}
=
\frac{1}{n}\sum_{i=1}^n \hat Y_i
=
\frac{1}{n}\sum_{i=1}^n (\hat\beta_0+\hat\beta_1X_i)
=
\hat\beta_0+\hat\beta_1\bar X.
\]</span></p>
<p>But since <span class="math inline">\(\hat\beta_0=\bar Y-\hat\beta_1\bar X\)</span>,
<span class="math display">\[
\overline{\hat Y}=\bar Y.
\]</span>
Equivalently, at <span class="math inline">\(X=\bar X\)</span>,
<span class="math display">\[
\hat Y(\bar X)=\hat\beta_0+\hat\beta_1\bar X=\bar Y.
\]</span></p>
<p>So the OLS fitted line passes through the point <span class="math inline">\((\bar X,\bar Y)\)</span>.</p>
</div>
<div id="mean-residual-is-zero" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> 4.2 Mean residual is zero<a class="anchor" aria-label="anchor" href="#mean-residual-is-zero"><i class="fas fa-link"></i></a>
</h3>
<p>Since <span class="math inline">\(\sum_i \hat u_i=0\)</span>,
<span class="math display">\[
\boxed{\bar{\hat u}=0.}
\]</span></p>
<hr>
</div>
</div>
<div id="step-5-decomposition-of-variation-tss-ess-rss" class="section level2" number="3.6">
<h2>
<span class="header-section-number">3.6</span> Step 5: Decomposition of variation (TSS = ESS + RSS)<a class="anchor" aria-label="anchor" href="#step-5-decomposition-of-variation-tss-ess-rss"><i class="fas fa-link"></i></a>
</h2>
<p>Define:
- Total sum of squares (TSS):
<span class="math display">\[
\text{TSS}=\sum_{i=1}^n (Y_i-\bar Y)^2.
\]</span>
- Explained sum of squares (ESS):
<span class="math display">\[
\text{ESS}=\sum_{i=1}^n (\hat Y_i-\bar Y)^2.
\]</span>
- Residual sum of squares (RSS):
<span class="math display">\[
\text{RSS}=\sum_{i=1}^n \hat u_i^2.
\]</span></p>
<p>Start from:
<span class="math display">\[
Y_i-\bar Y = (\hat Y_i-\bar Y) + (Y_i-\hat Y_i)
= (\hat Y_i-\bar Y) + \hat u_i.
\]</span></p>
<p>Square and sum:
<span class="math display">\[
\sum_{i=1}^n (Y_i-\bar Y)^2
=
\sum_{i=1}^n (\hat Y_i-\bar Y)^2
+
\sum_{i=1}^n \hat u_i^2
+
2\sum_{i=1}^n (\hat Y_i-\bar Y)\hat u_i.
\]</span></p>
<p>It remains to show the cross-term is zero. Note that
<span class="math display">\[
\hat Y_i-\bar Y
=
(\hat\beta_0+\hat\beta_1X_i)-\bar Y
=
\hat\beta_1(X_i-\bar X),
\]</span>
because <span class="math inline">\(\hat\beta_0=\bar Y-\hat\beta_1\bar X\)</span>. Therefore,
<span class="math display">\[
\sum_{i=1}^n (\hat Y_i-\bar Y)\hat u_i
=
\hat\beta_1\sum_{i=1}^n (X_i-\bar X)\hat u_i
=
\hat\beta_1\cdot 0
=0.
\]</span></p>
<p>So we obtain the classic identity:
<span class="math display">\[
\boxed{\text{TSS}=\text{ESS}+\text{RSS}.}
\]</span></p>
<hr>
</div>
<div id="step-6-sample-covariance-implications-useful-quick-corollaries" class="section level2" number="3.7">
<h2>
<span class="header-section-number">3.7</span> Step 6: Sample covariance implications (useful quick corollaries)<a class="anchor" aria-label="anchor" href="#step-6-sample-covariance-implications-useful-quick-corollaries"><i class="fas fa-link"></i></a>
</h2>
<p>Because <span class="math inline">\(\sum_i \hat u_i=0\)</span>, the sample covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(\hat u\)</span> is proportional to <span class="math inline">\(\sum_i (X_i-\bar X)\hat u_i\)</span>. Since that sum is zero,
<span class="math display">\[
\boxed{\widehat{\text{Cov}}(X,\hat u)=0.}
\]</span></p>
<p>Similarly, because <span class="math inline">\(\hat Y_i-\bar Y=\hat\beta_1(X_i-\bar X)\)</span>,
<span class="math display">\[
\sum_{i=1}^n \hat Y_i \hat u_i
=
\sum_{i=1}^n (\hat Y_i-\bar Y)\hat u_i + \bar Y\sum_{i=1}^n \hat u_i
=0+ \bar Y\cdot 0 = 0,
\]</span>
so
<span class="math display">\[
\boxed{\sum_{i=1}^n \hat Y_i \hat u_i=0.}
\]</span></p>
<hr>
</div>
<div id="optional-one-line-geometric-interpretation-if-you-want-it" class="section level2" number="3.8">
<h2>
<span class="header-section-number">3.8</span> Optional: one-line geometric interpretation (if you want it)<a class="anchor" aria-label="anchor" href="#optional-one-line-geometric-interpretation-if-you-want-it"><i class="fas fa-link"></i></a>
</h2>
<p>OLS chooses <span class="math inline">\(\hat Y\)</span> as the <strong>orthogonal projection</strong> of <span class="math inline">\(Y\)</span> onto the space spanned by <span class="math inline">\(\{1,X\}\)</span>. The residual vector <span class="math inline">\(\hat u\)</span> is orthogonal to that space, which is exactly why it is orthogonal to both <span class="math inline">\(1\)</span> and <span class="math inline">\(X\)</span>.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="tutorial-1-data-analysis-with-r.html"><span class="header-section-number">2</span> Tutorial 1: Data Analysis with R</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#tutorial-2-simple-linear-regression-with-intercept-ols-derivation-and-residual-properties"><span class="header-section-number">3</span> Tutorial 2: Simple Linear Regression (with Intercept): OLS Derivation and Residual Properties</a></li>
<li><a class="nav-link" href="#setup"><span class="header-section-number">3.1</span> Setup</a></li>
<li>
<a class="nav-link" href="#step-1-derive-the-ols-normal-equations"><span class="header-section-number">3.2</span> Step 1: Derive the OLS normal equations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#foc-for-beta_0"><span class="header-section-number">3.2.1</span> FOC for \(\beta_0\)</a></li>
<li><a class="nav-link" href="#foc-for-beta_1"><span class="header-section-number">3.2.2</span> FOC for \(\beta_1\)</a></li>
</ul>
</li>
<li><a class="nav-link" href="#step-2-derive-the-slope-in-centered-form"><span class="header-section-number">3.3</span> Step 2: Derive the slope in centered form</a></li>
<li>
<a class="nav-link" href="#step-3-residual-orthogonality-core-properties"><span class="header-section-number">3.4</span> Step 3: Residual orthogonality (core properties)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#residuals-sum-to-zero"><span class="header-section-number">3.4.1</span> 3.1 Residuals sum to zero</a></li>
<li><a class="nav-link" href="#residuals-are-orthogonal-to-x"><span class="header-section-number">3.4.2</span> 3.2 Residuals are orthogonal to \(X\)</a></li>
<li><a class="nav-link" href="#residuals-are-orthogonal-to-centered-regressor-x_i-bar-x"><span class="header-section-number">3.4.3</span> 3.3 Residuals are orthogonal to centered regressor \((X_i-\bar X)\)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#step-4-more-implications-involving-fitted-values-and-means"><span class="header-section-number">3.5</span> Step 4: More implications involving fitted values and means</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-regression-line-goes-through-bar-xbar-y"><span class="header-section-number">3.5.1</span> 4.1 The regression line goes through \((\bar X,\bar Y)\)</a></li>
<li><a class="nav-link" href="#mean-residual-is-zero"><span class="header-section-number">3.5.2</span> 4.2 Mean residual is zero</a></li>
</ul>
</li>
<li><a class="nav-link" href="#step-5-decomposition-of-variation-tss-ess-rss"><span class="header-section-number">3.6</span> Step 5: Decomposition of variation (TSS = ESS + RSS)</a></li>
<li><a class="nav-link" href="#step-6-sample-covariance-implications-useful-quick-corollaries"><span class="header-section-number">3.7</span> Step 6: Sample covariance implications (useful quick corollaries)</a></li>
<li><a class="nav-link" href="#optional-one-line-geometric-interpretation-if-you-want-it"><span class="header-section-number">3.8</span> Optional: one-line geometric interpretation (if you want it)</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Introduction to Econometrics</strong>" was written by Nicolás Campos Bijit. It was last built on 2026-01-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
