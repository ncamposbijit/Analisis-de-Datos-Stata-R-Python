[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"notes updated adapted course ECON 326: Introduction Econometrics II, University British Columbia (UBC), 2026.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"tutorial-1-data-analysis-with-r","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2 Tutorial 1: Data Analysis with R","text":"notes updated adapted course ECON 326: Introduction Econometrics II, undergraduate course University British Columbia (UBC), 2026. notes tutorial 1. introduction R R Studio.\n## Introduction R RStudioThis chapter introduces basic workflow working R RStudio. goal learn commands, also develop good habits writing clean, reproducible, well-documented code.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"why-use-r","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.0.1 Why use R?","text":"R one widely used programming languages data analysis Economics Social Sciences. main advantages :Open source: R free active community constantly develops new tools.Versatility: supports statistical econometric analysis, data cleaning, visualization, automation (including tasks like web scraping).Reproducibility: scripts reports allow document replicate results reliably.Labor market value: open-source programming languages increasingly demanded across research industry.course treat R “calculator”, programming language: write code, create objects, use functions systematically, build routines scale larger projects.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"key-concepts-vocabulary-of-the-course","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.0.2 Key concepts (vocabulary of the course)","text":"writing code, important define core terms appear throughout course:RStudio: Graphical User Interface (GUI) designed make working R easier.Objects: anything store memory (datasets, variables, lists, plots, models, etc.).Functions: operations take inputs return outputs (e.g., mean(x), lm(y ~ x)).Packages: collections functions grouped purpose (e.g., dplyr, ggplot2, rio).Scripts / Code: files containing commands define analysis pipeline.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"learning-resources","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.0.3 Learning resources","text":"learn R efficiently, rely three types resources: cheat sheets, forums, structured books.Cheat sheets: short summaries commands workflows.\nAvailable : https://www.rstudio.com/resources/cheatsheets/Forums websites:\nStack Overflow: https://stackoverflow.com/questions/tagged/r\nMedium: https://preettheman.medium.com/awesome-tricks-every-r-coder--know-c4220cd2cfbc\nPosit Blog (formerly RStudio): https://posit.co/\nStack Overflow: https://stackoverflow.com/questions/tagged/rMedium: https://preettheman.medium.com/awesome-tricks-every-r-coder--know-c4220cd2cfbcPosit Blog (formerly RStudio): https://posit.co/Books:\nR Data Science: https://r4ds..co.nz/\nPosit resources Spanish: https://posit.co/resources/videos/\nBookdown: https://bookdown.org/\nR Data Science: https://r4ds..co.nz/Posit resources Spanish: https://posit.co/resources/videos/Bookdown: https://bookdown.org/","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"rstudio-interface","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.0.4 RStudio interface","text":"RStudio convenient “vehicle” working R. interface usually displays four main panels, help organize work:Scripts (Source): write save code.Console: R runs commands prints results.Environment / History: see objects created memory.Files / Plots / Packages / Help: tools navigate folders, view plots, manage packages, read documentation.key takeaway work live Script, Console. Console useful quick checks, scripts ensure reproducibility.can customize RStudio via: Tools → Global Options (font size, theme, layout, etc.). purpose make workflow intuitive efficient possible.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"writing-code-comments-structure-and-cleaning-the-environment","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.1 Writing code: comments, structure and cleaning the environment","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"comments","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.1.1 Comments","text":"Good code code people (including “future ”) can understand. Comments essential explain something, .write comments R scripts, use #. comment multiple lines use:Ctrl + Shift + C (Windows)Cmd + Shift + C (Mac)useful reference best practices :Code Data Social Sciences: Practitioner’s Guide","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"outlines-code-indexing","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.1.2 Outlines (code indexing)","text":"RStudio allows structure scripts using headings. creates outline helps navigation long files.Open outline: Ctrl + Shift + OHeadings can hierarchical:Keeping outline updated optional: script grows, outline becomes one main tools navigate code efficiently. headings outdated inconsistent, outline loses usefulness long scripts become harder maintain.","code":"\n# The most important\n## This is a bit less important\n### This is a bit less important than the previous one\n#### The least important"},{"path":"tutorial-1-data-analysis-with-r.html","id":"cleaning-in-r-environment-vs-console","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.1.3 Cleaning in R: environment vs console","text":"working R, important distinguish two different things:environment (memory): R stores objects create (datasets, vectors, models, plots, etc.).console (screen): commands results printed.Cleaning environment deleting objects stored memory.\nCleaning console making screen look “clean” — delete anything.Remove objects environmentRemove one object","code":"\nrm(list = ls())\n# rm(data1)"},{"path":"tutorial-1-data-analysis-with-r.html","id":"package-installation-libraries-and-help","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.2 Package installation, libraries and help","text":"R, tools use contained packages. package simply collection functions designed specific purpose (e.g., data cleaning, plotting, importing files, estimation).key distinction following:Packages installed computer.Packages must loaded every time open new R session order use .difference crucial: many beginner errors come confusing “installing” “loading”.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"libraries","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.2.1 Libraries","text":"two standard ways install packages RStudio:Using Packages tab bottom-right panel.Using commands directly R.Many packages time:Common error:general, good idea install beginning work, since commonly use libraries data analysis. function, tell R install package installed (something typical switch computers):Load libraries:want see inside package:Important: package must installed , loaded every time used. often updates. check install :Alternatively, can link package function using ::. , necessary load library order use specific function. However, recommended practice load libraries packages use beginning.","code":"\ninstall.packages(\"dplyr\")\ninstall.packages(c(\"dplyr\",\"ggplot2\",\"rio\"))\ninstall.packages(dplyr, \"ggplot2\")\nif(!require(dplyr)) {install.packages(\"dplyr\")}\nlibrary(dplyr)\nls(\"package:dplyr\", all = TRUE) #ls = list objects\nupdate.packages()"},{"path":"tutorial-1-data-analysis-with-r.html","id":"help-function","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.2.2 Help function","text":"specific function:specific package:","code":"\nhelp(mean) \n?mean \nmean \nsd\nhelp(\"dplyr\") \nlibrary(help=\"dplyr\")"},{"path":"tutorial-1-data-analysis-with-r.html","id":"useful-shortcuts","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.2.3 Useful Shortcuts","text":"Esc: interrupt current commandCtrl + s: savetab: autocompleteCtrl + Enter: run lineCtrl + Shift + C: comment/uncomment<-: Alt + - / option + -%>%: Ctrl + Shift + M (pipe)Ctrl + l: clear consoleCtrl + Alt + b: run everything (arrow keys console allow view recent commands used)Shift + lines: select multiple linesCtrl + f: find/replaceCtrl + Arrow (console): view previously used commands","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"identify-the-package-of-a-function","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.2.3.1 Identify the package of a function","text":"Sometimes want know package given function belongs . , check:\nhttps://sebastiansauer.github.io/finds_funs/\n`Note + sign appears console. cases, RStudio stops probably forgot ) #. must correct error, run code , press Esc console continue executing commands.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"r-markdown---reproducible-research","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.3 R Markdown - Reproducible research","text":"R Markdown one useful tools applied data analysis allows combine explanatory text executable code single document. key idea simple:write analysis , every time data changes, can re-run document automatically update tables, figures, results.makes work:Automatic: outputs generated code (manually copied).Reproducible: anyone can re-run document obtain results.Well documented: results always accompanied explanations, interpretation, methodology.practice, R Markdown file (.Rmd) works like hybrid report script: contains written text (like Word document) chunks code (like R script). compile (“knit”) file, R executes code chunks inserts results (tables, plots, numbers) directly final report.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"what-can-r-markdown-produce","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.3.1 What can R Markdown produce?","text":".Rmd file can generate many different types outputs, :Word reports (.docx)PDF reports (.pdf)HTML reports (.html)Presentations (slides)Dashboards (interactive documents)importantly, .Rmd file can include elements typically appear data analysis:plain text explanationsR code chunkstablesplotsregression outputautomatically updated statistics (means, SDs, p-values, etc.)","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"when-is-r-markdown-especially-useful","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.3.2 When is R Markdown especially useful?","text":"R Markdown particularly valuable two common situations:Routine reports\nExample: weekly report updated descriptive statistics graphs.\nInstead rewriting report every week, update data re-knit.Routine reports\nExample: weekly report updated descriptive statistics graphs.\nInstead rewriting report every week, update data re-knit.Reports subsets dataset\nExample: dataset contains several countries. want one report per country.\nR Markdown, can parameterize report automatically generate one output per country.Reports subsets dataset\nExample: dataset contains several countries. want one report per country.\nR Markdown, can parameterize report automatically generate one output per country.general lesson : repeating work manually, can probably automated R Markdown.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"basic-concepts","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4 Basic concepts","text":"understand R Markdown, need distinguish following components:","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"markdown-.md","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.1 Markdown (.md)","text":"Markdown simple plain-text language format documents. example:# Title creates section title**bold** produces bold textlists, links, images easy writeMarkdown files extension .md.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"r-markdown-.rmd","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.2 R Markdown (.Rmd)","text":"R Markdown extends Markdown allowing embed R code directly document. Files extension .Rmd.important feature R code executed output included report. makes document dynamic reproducible.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"knitr","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.3 knitr","text":"knitr R package responsible :reading .Rmd fileidentifying code chunksexecuting R codeinserting results (tables/figures/output) documentIn words: knitr engine turns .Rmd file report.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"pandoc","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.4 Pandoc","text":"Pandoc tool converts report final format PDF, Word, HTML.takes document produced knitr transforms clean formatted file.\nPandoc installed automatically RStudio, need install manually.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"process-how-everything-works-together","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.5 Process (how everything works together)","text":"workflow :write text R code single .Rmd document.knitr executes code chunks.Pandoc converts final output PDF/Word/HTML.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"your-first-r-markdown-file","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.6 Your first R Markdown file","text":"create first .Rmd document RStudio:Go : File → New File → R MarkdownChoose output format: report / presentation / dashboardSet document title authorThe common starting point simple report outputs HTML Word.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"working-directory-critical-detail","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.7 Working directory (critical detail)","text":"frequent source confusion R Markdown working directory.general rule :working directory .Rmd file folder .Rmd file saved.means .Rmd file saved folder, R look datasets external files folder (unless specify full paths).course, keep things simple:\n- store data folder .Rmd file\n- avoid complicated paths gain familiarityThis best practice beginners reduces file-loading errors.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"components-of-an-r-markdown-file","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.8 Components of an R Markdown file","text":"R Markdown file three main components:YAML header\nblock top file containing metadata (title, author, output type).YAML header\nblock top file containing metadata (title, author, output type).Markdown text\nwrite narrative explanations: motivation, steps, interpretation.Markdown text\nwrite narrative explanations: motivation, steps, interpretation.Code chunks\nBlocks R code used loading packages, importing data, transforming variables, estimating models, generating plots.Code chunks\nBlocks R code used loading packages, importing data, transforming variables, estimating models, generating plots.code chunk looks like :typical workflow :YAML sets format + structureMarkdown text explains doingcode chunks analysis generate outputs","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"why-is-this-powerful","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.9 Why is this powerful?","text":"Word/Excel workflows, people often:\n- run regressions one place,\n- copy results document,\n- manually update tables plots.slow error-prone.R Markdown:\n- report generated directly code,\n- results always consistent data,\n- manual copy/paste.R Markdown considered core tool reproducible research.### Working directoryThe working directory .Rmd file folder file saved.Therefore, R search files folder .Rmd file stored.exercise, simply keep data use folder .Rmd file.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"components-of-r-markdown","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.4.10 Components of R Markdown","text":"YAML: set title, date, output type.Markdown text: write narrative text.Code chunk: load packages, data, create visualizations.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"object-manipulation","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.5 Object manipulation","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"using-r-as-a-calculator-running-commands","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.5.0.1 Using R as a calculator / running commands","text":"can run commands R interactive way, just like using calculator. , select line(s) code want execute press:Ctrl + Enter (Windows)Cmd + Enter (Mac)allows run commands separately (one line time selected block), without executing entire script.","code":"\n2+2 ## [1] 4\n3*5^(1/2)## [1] 6.708204"},{"path":"tutorial-1-data-analysis-with-r.html","id":"running-all-instructions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.5.0.2 Running all instructions","text":"execute entire script (.e., run commands file ), several options:Click Source (top-right Script panel) run whole script.Use menu: Code → Run Region → Run .Shortcut: Ctrl + Alt + R (Windows) / Cmd + Option + R (Mac)Running full script useful want reproduce complete workflow start finish (e.g., cleaning data, generating tables, producing plots).Even large operations can executed way. example, can select run full block code (data imports, transformations, loops, regressions, plots) instead running commands one one. especially useful analysis requires multiple steps must executed correct order.can even work complex numbers (imaginary numbers) R.\nR, imaginary unit written , complex numbers can used arithmetic operations just like real numbers.Example:","code":"\n2+2 ; 3*5^(1/2)## [1] 4## [1] 6.708204\n3+4 ## [1] 7\n5*4 ## [1] 20\n8/4 ## [1] 2\n6^7## [1] 279936\n6^77## [1] 8.272681e+59\nlog(10) ## [1] 2.302585\nlog(1)## [1] 0\nsqrt(91) # raiz cuadrada## [1] 9.539392\nround(7.3) # redondear## [1] 7\nsqrt(91) + 4892788*673 - (log(4)*round(67893.9181, digits = 2))## [1] 3292752213\n2i+5i+sqrt(25i)## [1] 3.53553+10.53553i"},{"path":"tutorial-1-data-analysis-with-r.html","id":"object-creation-assignment-and-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.5.0.3 Object creation: assignment and functions","text":"R, create objects using assignment operator <-. one important operations language, almost everything R consists creating transforming objects (vectors, datasets, models, plots, etc.).also possible assign values using =, recommended general practice, since can confusing (especially inside functions). recommended standard applied work use <-.","code":"\ny <- 2 + 4 \ny## [1] 6"},{"path":"tutorial-1-data-analysis-with-r.html","id":"understanding-assignments-and-functions-in-r","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6 Understanding Assignments and Functions in R","text":"Assignments R silent operations. create assignment, stores value automatically display console. see result assignment, must explicitly call object typing name using print() function.simple assignments straightforward, real power R lies generating assignments functions. Functions central component working R. functions come pre-installed base R installation, others must obtained external packages. Additionally, users can write custom functions. Functions typically written parentheses, filter(). cases, functions associated specific packages referenced using double colon notation, example dplyr::filter().Let’s explore functions work several examples. First, can apply simple mathematical function:calculates square root 49. can also apply functions datasets. instance, can obtain summary statistics variable within dataset:mtcars dataset comes pre-installed R contains information various car models. see datasets come bundled R, can use data() function:","code":"\nsqrt(49)## [1] 7\nsummary(mtcars$mpg)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\ndata()"},{"path":"tutorial-1-data-analysis-with-r.html","id":"working-with-vectors-and-arithmetic-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.1 Working with Vectors and Arithmetic Functions","text":"can create objects assigning values combine objects vectors. vector fundamental data structure R contains elements type:, create two individual numeric objects (x y) combine vector z using concatenate function c(). vector, can apply various statistical functions :functions calculate arithmetic mean median values vector, respectively.","code":"\nx <- 2\ny <- 3\nz <- c(x, y)\nz## [1] 2 3\nmean(z)## [1] 2.5\nmedian(z)## [1] 2.5"},{"path":"tutorial-1-data-analysis-with-r.html","id":"creating-relationships-between-objects","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.2 Creating Relationships Between Objects","text":"can create new objects based calculations performed existing objects. example, can store mean vector new object:can also create objects arithmetic operations:R allows us perform logical comparisons objects:returns logical value (TRUE FALSE) indicating whether greater b.","code":"\nw <- mean(z)\na <- 3 + 10\nb <- 2 * 4\na > b## [1] TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"the-silent-nature-of-assignments","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.3 The Silent Nature of Assignments","text":"mentioned earlier, assignments produce output unless explicitly request . display contents object, simply type name use print() function:","code":"\na## [1] 13\nb## [1] 8\n# alternatively, use print\nprint(a)## [1] 13\nprint(b)## [1] 8"},{"path":"tutorial-1-data-analysis-with-r.html","id":"building-complex-objects-from-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.4 Building Complex Objects from Functions","text":"can build complex objects combining multiple operations. instance, can create vector previously defined objects calculate mean:Let’s work another example calculate mean two values:clear workspace remove specific objects, can use rm() function:first command removes objects environment, second removes specific object.","code":"\nvalores <- c(a, b)\npromedio <- mean(valores)\nprint(promedio)## [1] 10.5\npromedio## [1] 10.5\na <- 2\nb <- 5\nvalores1 <- c(a, b)\npromedio1 <- mean(valores1)\nprint(promedio1)## [1] 3.5\nrm(list = ls())\nrm(promedio)## Warning in rm(promedio): object 'promedio' not found"},{"path":"tutorial-1-data-analysis-with-r.html","id":"working-with-real-data-education-and-income-example","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.5 Working with Real Data: Education and Income Example","text":"Proper spacing organization code essential readability maintenance. Let’s create two vectors representing education (years) income ten individuals using concatenate function c():vectors, can calculate various descriptive statistics. can compute mean income, standard deviation income, correlation education income:mean provides average income, standard deviation measures dispersion income values around mean, correlation coefficient quantifies strength direction linear relationship education income.","code":"\neduc <- c(8, 12, 8, 11, 16, 14, 8, 10, 14, 12)\ningreso <- c(325, 415, 360, 380, 670, 545, 350, 420, 680, 465)\nmean(ingreso)## [1] 461\npromedioingreso <- mean(ingreso)\n\nsd(ingreso)## [1] 129.1382\nsdingreso <- sd(ingreso)\n\ncor(educ, ingreso)## [1] 0.9110521\ncoreduing <- cor(educ, ingreso)"},{"path":"tutorial-1-data-analysis-with-r.html","id":"visualizing-the-relationship","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.6 Visualizing the Relationship","text":"visually examine relationship education income, can create scatter plot:produces graph education x-axis income y-axis, allowing us see pattern association two variables.","code":"\nplot(educ, ingreso)"},{"path":"tutorial-1-data-analysis-with-r.html","id":"estimating-a-linear-regression-model","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.6.7 Estimating a Linear Regression Model","text":"Finally, can formally model relationship education income using linear regression. lm() function estimates linear model income dependent variable education independent variable:function returns estimated coefficients linear model, providing us intercept slope coefficient education. slope coefficient indicates much income expected change additional year education.","code":"\nlm(ingreso ~ educ)## \n## Call:\n## lm(formula = ingreso ~ educ)\n## \n## Coefficients:\n## (Intercept)         educ  \n##       -8.71        41.57"},{"path":"tutorial-1-data-analysis-with-r.html","id":"object-naming-and-types","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7 Object Naming and Types","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"naming-objects","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.1 Naming Objects","text":"following exercise helps us understand rules valid variable names R.Exercise: Valid Variable NamesConsider following examples. valid variable names R?R, variable names must follow rules: can contain letters, numbers, dots, underscores, must start letter dot (starting dot, second character number). Variable names contain spaces special characters like hyphens, start numbers underscores.","code":"\n# min_height      # Valid: uses underscore\n# max.height      # Valid: uses dot\n# _age            # Invalid: starts with underscore\n# .mass           # Valid: starts with dot (but creates hidden variable)\n# MaxLength       # Valid: uses camel case\n# Min-length      # Invalid: uses hyphen (minus sign)\n# 2widths         # Invalid: starts with number\n# Calsius2kelvin  # Valid: number in middle is allowed"},{"path":"tutorial-1-data-analysis-with-r.html","id":"types-of-objects","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2 Types of Objects","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"vectors","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.1 Vectors","text":"R operates component component, makes straightforward work vectors matrices. Vectors one-dimensional arrays can hold numeric data, character data, logical data, elements must type.create vector, use c() function (stands “concatenate” “combine”):Let’s examine happens add vectors together:Now, let’s consider two vectors different lengths:can check length vector using length() function:happens try add vectors different lengths?IMPORTANT: case, R performs operation anyway, gives us warning lengths differ. R uses “vector recycling,” shorter vector repeated match length longer vector. crucial characteristic vectors can concatenate elements type; otherwise, R coerce elements common type (usually character ’s mix).Let’s explore vector operations:","code":"\nx <- c(1, 2, 3, 4, 5)\n# or alternatively\ny <- c(6:8)\nz <- x + y## Warning in x + y: longer object length is not a multiple of shorter object length\nz## [1]  7  9 11 10 12\nx <- c(1:4)\ny <- c(1:3)\nlength(x)## [1] 4\nlength(y)## [1] 3\nz <- x + y## Warning in x + y: longer object length is not a multiple of shorter object length\nz## [1] 2 4 6 5\nx <- rep(1.5:9.5, 4)  # generates repetitions of the defined values\ny <- c(20:30)\nx1 <- c(1, 2)\nx2 <- c(3, 4)\nx3 <- c(x1, x2)\nx4 <- c(c(1, 2), c(3, 4))"},{"path":"tutorial-1-data-analysis-with-r.html","id":"subsetting-vectors","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.1.1 Subsetting Vectors","text":"can extract specific elements vector using square bracket notation:","code":"\ny[3]         # get the third element## [1] 22\ny[2:4]       # get elements 2 through 4## [1] 21 22 23\ny[4:2]       # get elements in reverse order (4, 3, 2)## [1] 23 22 21\ny[c(2, 6)]   # get elements 2 and 6## [1] 21 25\ny[c(2, 16)]  # attempt to get element 16 (returns NA if out of bounds)## [1] 21 NA"},{"path":"tutorial-1-data-analysis-with-r.html","id":"matrices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2 Matrices","text":"Matrices two-dimensional arrays elements must type. particularly useful mathematical operations organizing data rows columns.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"defining-matrices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.1 Defining Matrices","text":"general syntax creating matrix :create matrices, use matrix() function:necessary explicitly write data=, improves code readability makes intent clearer.Note DEFAULT, R fills matrix column column. can explicitly specify want fill matrix row row using byrow argument:can determine dimensions matrix using dim() function:vector shorter matrix dimensions, R recycle values fill matrix:Note order matrix always rows × columns. can also omit either number rows columns, R calculate missing dimension:creating empty matrices, must define dimensions:","code":"\nmy.matrix <- matrix(vector,\n                    ncol = num_columns,\n                    nrow = num_rows,\n                    byrow = logical_value,\n                    dimnames = list(vector_row_names,\n                                   vector_column_names))\nx <- matrix(data = c(1, 2, 3, 4),\n            nrow = 2,\n            ncol = 2)\n\nx1 <- matrix(c(1, 2, 3, 4),\n             2,\n             2)\nx##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nx1##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\ny <- matrix(data = c(1:4),\n            nrow = 3,\n            ncol = 2,\n            byrow = TRUE)## Warning in matrix(data = c(1:4), nrow = 3, ncol = 2, byrow = TRUE): data length [4] is not a sub-multiple or multiple of the number\n## of rows [3]\ny##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\n## [3,]    1    2\ndim(y)## [1] 3 2\ndim(y)[1]  # number of rows## [1] 3\ndim(y)[2]  # number of columns## [1] 2\ny <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = 2)\ny##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\ny <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 3, byrow = 2)## Warning in matrix(c(1, 2, 3, 4), nrow = 2, ncol = 3, byrow = 2): data length [4] is not a sub-multiple or multiple of the number of\n## columns [3]\ny##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    1    2\ny <- matrix(c(1:4), 2, byrow = T)\ny##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\ny <- matrix(nrow = 3, ncol = 3)\ny  # useful for loops where we fill values iteratively##      [,1] [,2] [,3]\n## [1,]   NA   NA   NA\n## [2,]   NA   NA   NA\n## [3,]   NA   NA   NA"},{"path":"tutorial-1-data-analysis-with-r.html","id":"naming-rows-and-columns","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.2 Naming Rows and Columns","text":"can assign names rows columns directly matrix() function:Alternatively, can use colnames() rownames() functions:","code":"\ny <- matrix(c(1:4), 2, byrow = FALSE,\n            dimnames = list(c(\"X1\", \"X2\"), c(\"Y1\", \"Y2\")))\ny##    Y1 Y2\n## X1  1  3\n## X2  2  4\ncolnames(x) <- c(\"Variable 1\", \"Variable 2\")\nrownames(x) <- c(\"a1\", \"a2\")\nx##    Variable 1 Variable 2\n## a1          1          3\n## a2          2          4"},{"path":"tutorial-1-data-analysis-with-r.html","id":"adding-rows-or-columns-to-a-matrix","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.3 Adding Rows or Columns to a Matrix","text":"Let’s create new vector add matrix:can combine matrices vectors using rows (note vector name becomes row name):can also combine using columns:happens different numbers rows /columns? R recycle shorter vector observation match longer dimension:","code":"\nw <- c(5, 6)\nz <- rbind(x, w)\nz##    Variable 1 Variable 2\n## a1          1          3\n## a2          2          4\n## w           5          6\nz <- cbind(x, w)\nz##    Variable 1 Variable 2 w\n## a1          1          3 5\n## a2          2          4 6\nx <- matrix(c(1:9), 3)\nx##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\ny <- c(5, 6)\ny## [1] 5 6\nz <- rbind(x, y)## Warning in rbind(x, y): number of columns of result is not a multiple of vector length (arg 2)\nz##   [,1] [,2] [,3]\n##      1    4    7\n##      2    5    8\n##      3    6    9\n## y    5    6    5"},{"path":"tutorial-1-data-analysis-with-r.html","id":"converting-vectors-to-matrices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.4 Converting Vectors to Matrices","text":"can convert vector matrix assigning dimensions:","code":"\nx <- 1:10\nx##  [1]  1  2  3  4  5  6  7  8  9 10\ndim(x) <- c(2, 5)\nx##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    3    5    7    9\n## [2,]    2    4    6    8   10"},{"path":"tutorial-1-data-analysis-with-r.html","id":"transposing-matrices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.5 Transposing Matrices","text":"can transpose matrix (swap rows columns) using t() function:","code":"\nx <- matrix(c(1:9), 3)\nx_transposed <- t(x)\nx_transposed##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9"},{"path":"tutorial-1-data-analysis-with-r.html","id":"subsetting-matrices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.7.2.2.6 Subsetting Matrices","text":"can extract specific elements, rows, columns matrix. example, get second fourth elements second row:Matrix subsetting uses format [row, column], leaving either position blank returns rows columns, respectively.","code":"\nM <- matrix(1:8, nrow = 2)\nM##      [,1] [,2] [,3] [,4]\n## [1,]    1    3    5    7\n## [2,]    2    4    6    8\nM[1, 1]       # element in first row, first column## [1] 1\nM[1, ]        # entire first row## [1] 1 3 5 7\nM[, 2]        # entire second column## [1] 3 4\nM[2, c(2, 4)] # second and fourth elements of the second row## [1] 4 8"},{"path":"tutorial-1-data-analysis-with-r.html","id":"arrays","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.8 Arrays","text":"Arrays multidimensional data structures extend concept matrices. matrices two-dimensional, arrays can three dimensions. particularly useful working data naturally multiple dimensions, time series data across different regions variables.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"creating-arrays","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.8.1 Creating Arrays","text":"basic syntax creating array : array(data, dim, dimnames)array dimensions 2 × 4 × 3, meaning 2 rows, 4 columns, 3 “layers” levels third dimension.","code":"\n# Creating a simple 3D array\nmy_array <- array(1:24, c(2, 4, 3))\nmy_array## , , 1\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]    1    3    5    7\n## [2,]    2    4    6    8\n## \n## , , 2\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]    9   11   13   15\n## [2,]   10   12   14   16\n## \n## , , 3\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]   17   19   21   23\n## [2,]   18   20   22   24"},{"path":"tutorial-1-data-analysis-with-r.html","id":"adding-dimension-names","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.8.2 Adding Dimension Names","text":"Dimension names make arrays much easier interpret work :","code":"\n# Define names for each dimension\ndim1 <- c(\"row1\", \"row2\")\ndim2 <- c(\"col1\", \"col2\", \"col3\", \"col4\")\ndim3 <- c(\"level1\", \"level2\", \"level3\")\n\n# Create array with named dimensions\nmy_array <- array(1:24, c(2, 4, 3),\n                  dimnames = list(dim1, dim2, dim3))\nmy_array## , , level1\n## \n##      col1 col2 col3 col4\n## row1    1    3    5    7\n## row2    2    4    6    8\n## \n## , , level2\n## \n##      col1 col2 col3 col4\n## row1    9   11   13   15\n## row2   10   12   14   16\n## \n## , , level3\n## \n##      col1 col2 col3 col4\n## row1   17   19   21   23\n## row2   18   20   22   24"},{"path":"tutorial-1-data-analysis-with-r.html","id":"accessing-array-elements","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.8.3 Accessing Array Elements","text":"Arrays can subset using square brackets, similar matrices additional dimensions:","code":"\n# Access a single element: row 1, column 2, level 3\nmy_array[1, 2, 3]## [1] 19\n# Access entire slices\nmy_array[, , 1]  # All of level 1##      col1 col2 col3 col4\n## row1    1    3    5    7\n## row2    2    4    6    8\nmy_array[1, , ]  # All of row 1 across all levels##      level1 level2 level3\n## col1      1      9     17\n## col2      3     11     19\n## col3      5     13     21\n## col4      7     15     23\n# Access by dimension names\nmy_array[\"row1\", \"col2\", \"level3\"]## [1] 19"},{"path":"tutorial-1-data-analysis-with-r.html","id":"lists","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.9 Lists","text":"Lists R’s flexible data structure. Unlike vectors, must contain elements type, lists can contain elements different types, including lists, vectors, matrices, dataframes, even functions.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"creating-lists","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.9.1 Creating Lists","text":"","code":"\n# Create a heterogeneous list\nmy_list <- list(\n  numbers = c(1, 2, 3),\n  text = \"Hello R\",\n  matrix = matrix(1:6, nrow = 2),\n  flag = TRUE\n)\n\nmy_list## $numbers\n## [1] 1 2 3\n## \n## $text\n## [1] \"Hello R\"\n## \n## $matrix\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n## \n## $flag\n## [1] TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"accessing-list-components","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.9.2 Accessing List Components","text":"Lists three main ways access components:","code":"\n# Method 1: Double brackets [[ ]] - extracts the element itself\nmy_list[[1]]  # Returns the numeric vector## [1] 1 2 3\n# Method 2: Dollar sign $ - access by name\nmy_list$text  # Returns \"Hello R\"## [1] \"Hello R\"\n# Method 3: Single brackets [ ] - returns a sublist\nmy_list[1]    # Returns a list containing the first element## $numbers\n## [1] 1 2 3\n# Accessing nested elements\nmy_list[[3]][1, 2]  # Second element of first row in the matrix## [1] 3"},{"path":"tutorial-1-data-analysis-with-r.html","id":"adding-and-removing-list-elements","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.9.3 Adding and Removing List Elements","text":"","code":"\n# Adding new components\nmy_list$new_vector <- c(10, 20, 30)\n\n# Removing components (set to NULL)\nmy_list$flag <- NULL\n\n# View updated structure\nstr(my_list)## List of 4\n##  $ numbers   : num [1:3] 1 2 3\n##  $ text      : chr \"Hello R\"\n##  $ matrix    : int [1:2, 1:3] 1 2 3 4 5 6\n##  $ new_vector: num [1:3] 10 20 30"},{"path":"tutorial-1-data-analysis-with-r.html","id":"data-frames","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.10 Data Frames","text":"Data frames workhorse data structure statistical analysis R. similar matrices can contain columns different types, making perfect representing datasets variables may numeric, character, logical.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"creating-data-frames","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.10.1 Creating Data Frames","text":"","code":"\n# Create a data frame\ndf <- data.frame(\n  ID = 1:5,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  Age = c(25, 30, 35, 28, 32),\n  Employed = c(TRUE, TRUE, FALSE, TRUE, TRUE)\n)\n\ndf##   ID    Name Age Employed\n## 1  1   Alice  25     TRUE\n## 2  2     Bob  30     TRUE\n## 3  3 Charlie  35    FALSE\n## 4  4   Diana  28     TRUE\n## 5  5     Eve  32     TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"exploring-data-frame-structure","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.10.2 Exploring Data Frame Structure","text":"","code":"\n# View structure\nstr(df)## 'data.frame':    5 obs. of  4 variables:\n##  $ ID      : int  1 2 3 4 5\n##  $ Name    : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\" ...\n##  $ Age     : num  25 30 35 28 32\n##  $ Employed: logi  TRUE TRUE FALSE TRUE TRUE\n# Dimensions\ndim(df)## [1] 5 4\nnrow(df)## [1] 5\nncol(df)## [1] 4\n# Column names\nnames(df)## [1] \"ID\"       \"Name\"     \"Age\"      \"Employed\"\ncolnames(df)## [1] \"ID\"       \"Name\"     \"Age\"      \"Employed\"\n# Summary statistics\nsummary(df)##        ID        Name                Age      Employed      \n##  Min.   :1   Length:5           Min.   :25   Mode :logical  \n##  1st Qu.:2   Class :character   1st Qu.:28   FALSE:1        \n##  Median :3   Mode  :character   Median :30   TRUE :4        \n##  Mean   :3                      Mean   :30                  \n##  3rd Qu.:4                      3rd Qu.:32                  \n##  Max.   :5                      Max.   :35\n# First and last rows\nhead(df, 3)##   ID    Name Age Employed\n## 1  1   Alice  25     TRUE\n## 2  2     Bob  30     TRUE\n## 3  3 Charlie  35    FALSE\ntail(df, 2)##   ID  Name Age Employed\n## 4  4 Diana  28     TRUE\n## 5  5   Eve  32     TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"subsetting-data-frames","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.10.3 Subsetting Data Frames","text":"Data frames combine subsetting behavior matrices lists:","code":"\n# Accessing columns\ndf$Name           # By name with $## [1] \"Alice\"   \"Bob\"     \"Charlie\" \"Diana\"   \"Eve\"\ndf[, \"Age\"]       # By name with brackets## [1] 25 30 35 28 32\ndf[, 3]           # By position## [1] 25 30 35 28 32\n# Accessing rows\ndf[2, ]           # Second row##   ID Name Age Employed\n## 2  2  Bob  30     TRUE\n# Accessing specific cells\ndf[2, 3]          # Row 2, Column 3## [1] 30\ndf[2, \"Age\"]      # Same, using column name## [1] 30\n# Subsetting with conditions\ndf[df$Age > 28, ]                    # Rows where Age > 28##   ID    Name Age Employed\n## 2  2     Bob  30     TRUE\n## 3  3 Charlie  35    FALSE\n## 5  5     Eve  32     TRUE\ndf[df$Employed == TRUE, c(\"Name\", \"Age\")]  # Employed people's names and ages##    Name Age\n## 1 Alice  25\n## 2   Bob  30\n## 4 Diana  28\n## 5   Eve  32"},{"path":"tutorial-1-data-analysis-with-r.html","id":"tibbles-modern-data-frames","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.10.4 Tibbles: Modern Data Frames","text":"tibble package (part tidyverse) provides enhanced version data frames:","code":"\nlibrary(tibble)\n\n# Create a tibble\ntb <- tibble(\n  ID = 1:5,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  Age = c(25, 30, 35, 28, 32),\n  Employed = c(TRUE, TRUE, FALSE, TRUE, TRUE)\n)\n\ntb## # A tibble: 5 × 4\n##      ID Name      Age Employed\n##   <int> <chr>   <dbl> <lgl>   \n## 1     1 Alice      25 TRUE    \n## 2     2 Bob        30 TRUE    \n## 3     3 Charlie    35 FALSE   \n## 4     4 Diana      28 TRUE    \n## 5     5 Eve        32 TRUE\n# Tibbles have better printing and more consistent behavior"},{"path":"tutorial-1-data-analysis-with-r.html","id":"data-types-and-type-checking","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11 Data Types and Type Checking","text":"Understanding data types crucial avoiding errors writing robust R code.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"main-data-types","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11.1 Main Data Types","text":"","code":"\n# Character\nchar_var <- \"Hello\"\nclass(char_var)## [1] \"character\"\n# Numeric (double precision)\nnum_var <- 3.14\nclass(num_var)## [1] \"numeric\"\n# Integer\nint_var <- 42L\nclass(int_var)## [1] \"integer\"\n# Logical\nlog_var <- TRUE\nclass(log_var)## [1] \"logical\"\n# Complex\ncomplex_var <- 3 + 2i\nclass(complex_var)## [1] \"complex\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"type-checking-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11.2 Type Checking Functions","text":"","code":"\nx <- 42\n\n# Specific type checks\nis.numeric(x)## [1] TRUE\nis.integer(x)## [1] FALSE\nis.character(x)## [1] FALSE\nis.logical(x)## [1] FALSE\n# Convert x to integer\nx_int <- as.integer(x)\nis.integer(x_int)## [1] TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"type-coercion","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11.3 Type Coercion","text":"R combines different types, follows coercion hierarchy:\nlogical → integer → numeric → complex → character","code":"\n# Combining logical and numeric\nc(TRUE, 1, 2)  # TRUE becomes 1## [1] 1 1 2\n# Combining numeric and character\nc(1, 2, \"three\")  # Numbers become characters## [1] \"1\"     \"2\"     \"three\"\n# Explicit coercion\nas.character(42)## [1] \"42\"\nas.numeric(\"3.14\")## [1] 3.14\nas.logical(1)  # Non-zero numbers become TRUE## [1] TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"missing-values","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11.4 Missing Values","text":"R uses NA (Available) represent missing values.","code":"\n# Creating vectors with missing values\nx <- c(1, 2, NA, 4, NA)\n\n# Detecting missing values\nis.na(x)## [1] FALSE FALSE  TRUE FALSE  TRUE\n# Counting missing values\nsum(is.na(x))## [1] 2\n# Removing missing values\nx[!is.na(x)]## [1] 1 2 4\nna.omit(x)## [1] 1 2 4\n## attr(,\"na.action\")\n## [1] 3 5\n## attr(,\"class\")\n## [1] \"omit\"\n# Many functions have na.rm parameter\nmean(x)              # Returns NA## [1] NA\nmean(x, na.rm = TRUE)  # Calculates mean ignoring NAs## [1] 2.333333"},{"path":"tutorial-1-data-analysis-with-r.html","id":"conditional-statements","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.11.5 Conditional Statements","text":"Conditional statements allow code make decisions execute different code based conditions.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"if-else-statements","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.12 If-Else Statements","text":"","code":"\n# Basic if statement\nx <- 10\n\nif (x > 5) {\n  print(\"x is greater than 5\")\n}## [1] \"x is greater than 5\"\n# If-else\nx <- 3\n\nif (x > 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is less than or equal to 5\")\n}## [1] \"x is less than or equal to 5\"\n# If-else if-else chain\nscore <- 75\n\nif (score >= 90) {\n  grade <- \"A\"\n} else if (score >= 80) {\n  grade <- \"B\"\n} else if (score >= 70) {\n  grade <- \"C\"\n} else {\n  grade <- \"F\"\n}\n\ngrade## [1] \"C\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"vectorized-if-else-ifelse","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.13 Vectorized If-Else: ifelse()","text":"ifelse() function vectorized, making perfect applying conditions entire vectors:","code":"\n# Create a vector of test scores\nscores <- c(92, 75, 88, 65, 95, 70)\n\n# Assign grades using ifelse\ngrades <- ifelse(scores >= 80, \"Pass\", \"Fail\")\ngrades## [1] \"Pass\" \"Fail\" \"Pass\" \"Fail\" \"Pass\" \"Fail\"\n# Nested ifelse for multiple conditions\ndetailed_grades <- ifelse(scores >= 90, \"A\",\n                   ifelse(scores >= 80, \"B\",\n                   ifelse(scores >= 70, \"C\", \"F\")))\ndetailed_grades## [1] \"A\" \"C\" \"B\" \"F\" \"A\" \"C\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.13.1 Functions","text":"Functions reusable blocks code perform specific tasks. fundamental writing clean, maintainable code.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"creating-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.13.2 Creating Functions","text":"","code":"\n# Basic function structure\nsquare <- function(x) {\n  result <- x^2\n  return(result)\n}\n\nsquare(4)## [1] 16\nsquare(1:5)## [1]  1  4  9 16 25"},{"path":"tutorial-1-data-analysis-with-r.html","id":"functions-with-multiple-arguments","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.13.3 Functions with Multiple Arguments","text":"","code":"\n# Function with default parameter values\npower <- function(base, exponent = 2) {\n  result <- base^exponent\n  return(result)\n}\n\npower(3)      # Uses default exponent = 2## [1] 9\npower(3, 3)   # Explicit exponent## [1] 27\npower(2, exponent = 4)  # Named argument## [1] 16"},{"path":"tutorial-1-data-analysis-with-r.html","id":"practical-example-statistical-summary-function","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.13.4 Practical Example: Statistical Summary Function","text":"","code":"\n# Create a function that returns multiple statistics\nsummary_stats <- function(x, na.rm = TRUE) {\n  list(\n    mean = mean(x, na.rm = na.rm),\n    median = median(x, na.rm = na.rm),\n    sd = sd(x, na.rm = na.rm),\n    min = min(x, na.rm = na.rm),\n    max = max(x, na.rm = na.rm),\n    n = length(x),\n    n_missing = sum(is.na(x))\n  )\n}\n\n# Test the function\ntest_data <- c(1, 2, 3, NA, 5, 6, 7, NA, 9, 10)\nsummary_stats(test_data)## $mean\n## [1] 5.375\n## \n## $median\n## [1] 5.5\n## \n## $sd\n## [1] 3.248626\n## \n## $min\n## [1] 1\n## \n## $max\n## [1] 10\n## \n## $n\n## [1] 10\n## \n## $n_missing\n## [1] 2"},{"path":"tutorial-1-data-analysis-with-r.html","id":"loops-and-iteration","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14 Loops and Iteration","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"for-loops","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.1 For Loops","text":"loops iterate sequence values:","code":"\n# Basic for loop\nfor (i in 1:5) {\n  print(paste(\"Iteration\", i))\n}## [1] \"Iteration 1\"\n## [1] \"Iteration 2\"\n## [1] \"Iteration 3\"\n## [1] \"Iteration 4\"\n## [1] \"Iteration 5\"\n# Iterating over vector elements\nfruits <- c(\"apple\", \"banana\", \"cherry\")\n\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}## [1] \"I like apple\"\n## [1] \"I like banana\"\n## [1] \"I like cherry\"\n# Building results in a loop\nsquares <- numeric(10)\nfor (i in 1:10) {\n  squares[i] <- i^2\n}\nsquares##  [1]   1   4   9  16  25  36  49  64  81 100"},{"path":"tutorial-1-data-analysis-with-r.html","id":"while-loops","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.2 While Loops","text":"loops continue condition becomes FALSE:","code":"\n# Basic while loop\ncounter <- 1\n\nwhile (counter <= 5) {\n  print(paste(\"Counter is\", counter))\n  counter <- counter + 1\n}## [1] \"Counter is 1\"\n## [1] \"Counter is 2\"\n## [1] \"Counter is 3\"\n## [1] \"Counter is 4\"\n## [1] \"Counter is 5\"\n# Practical example: finding first value above threshold\nx <- 1\nwhile (2^x < 1000) {\n  x <- x + 1\n}\nprint(paste(\"2^\", x, \"=\", 2^x, \"is the first power of 2 above 1000\"))## [1] \"2^ 10 = 1024 is the first power of 2 above 1000\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"apply-family-functions","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.3 Apply Family Functions","text":"apply family provides functional programming alternatives loops. often faster concise.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"lapply-apply-function-to-listvector","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.4 lapply: Apply Function to List/Vector","text":"lapply always returns list:","code":"\n# Apply function to each element\nnumbers <- list(a = 1:5, b = 6:10, c = 11:15)\n\n# Calculate mean of each element\nlapply(numbers, mean)## $a\n## [1] 3\n## \n## $b\n## [1] 8\n## \n## $c\n## [1] 13\n# Using anonymous function\nlapply(numbers, function(x) x^2)## $a\n## [1]  1  4  9 16 25\n## \n## $b\n## [1]  36  49  64  81 100\n## \n## $c\n## [1] 121 144 169 196 225\n# Practical example: read multiple files\n# file_names <- c(\"data1.csv\", \"data2.csv\", \"data3.csv\")\n# data_list <- lapply(file_names, read.csv)"},{"path":"tutorial-1-data-analysis-with-r.html","id":"sapply-simplified-apply","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.5 sapply: Simplified Apply","text":"sapply tries simplify result vector matrix:","code":"\n# Same operation, simplified output\nsapply(numbers, mean)##  a  b  c \n##  3  8 13\n# Returns vector instead of list\nsapply(numbers, function(x) sum(x^2))##   a   b   c \n##  55 330 855\n# When result can't be simplified, returns list like lapply\nsapply(numbers, function(x) c(min = min(x), max = max(x)))##     a  b  c\n## min 1  6 11\n## max 5 10 15"},{"path":"tutorial-1-data-analysis-with-r.html","id":"vapply-type-safe-apply","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.14.6 vapply: Type-Safe Apply","text":"vapply requires specify output type, making safer sometimes faster:","code":"\n# Specify output type\nvapply(numbers, mean, FUN.VALUE = numeric(1))##  a  b  c \n##  3  8 13\n# For multiple return values\nvapply(numbers, function(x) c(mean = mean(x), sd = sd(x)),\n       FUN.VALUE = numeric(2))##             a        b         c\n## mean 3.000000 8.000000 13.000000\n## sd   1.581139 1.581139  1.581139\n# Type safety prevents errors\n# This would error if we tried to return character when numeric specified"},{"path":"tutorial-1-data-analysis-with-r.html","id":"comparison-and-best-practices","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.15 Comparison and Best Practices","text":"Best Practices:\n- Use lapply need list output applying complex functions\n- Use sapply interactive analysis want simplified output\n- Use vapply production code packages type safety important","code":"\n# Create example data\nmy_list <- list(\n  group1 = rnorm(100, mean = 10, sd = 2),\n  group2 = rnorm(100, mean = 15, sd = 3),\n  group3 = rnorm(100, mean = 20, sd = 4)\n)\n\n# lapply: Always returns list\nresult_l <- lapply(my_list, summary)\nclass(result_l)## [1] \"list\"\n# sapply: Simplifies if possible\nresult_s <- sapply(my_list, mean)\nclass(result_s)## [1] \"numeric\"\n# vapply: Type-safe, better for production code\nresult_v <- vapply(my_list, mean, FUN.VALUE = numeric(1))\nclass(result_v)## [1] \"numeric\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"practical-applications","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.16 Practical Applications","text":"","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"example-1-data-cleaning-pipeline","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.16.1 Example 1: Data Cleaning Pipeline","text":"","code":"\n# Create sample data with issues\nmessy_data <- data.frame(\n  id = 1:6,\n  value = c(10, 20, NA, 40, 50, 60),\n  category = c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"),\n  valid = c(TRUE, TRUE, FALSE, TRUE, TRUE, TRUE)\n)\n\n# Function to clean and summarize\nclean_and_summarize <- function(df) {\n  # Remove invalid rows\n  df_clean <- df[df$valid == TRUE, ]\n\n  # Remove missing values\n  df_clean <- df_clean[!is.na(df_clean$value), ]\n\n  # Calculate summary by category\n  categories <- unique(df_clean$category)\n  results <- lapply(categories, function(cat) {\n    subset_data <- df_clean[df_clean$category == cat, \"value\"]\n    c(\n      category = cat,\n      mean = mean(subset_data),\n      n = length(subset_data)\n    )\n  })\n\n  # Combine into data frame\n  do.call(rbind, results)\n}\n\nclean_and_summarize(messy_data)##      category mean n  \n## [1,] \"A\"      \"35\" \"2\"\n## [2,] \"B\"      \"35\" \"2\"\n## [3,] \"C\"      \"40\" \"1\""},{"path":"tutorial-1-data-analysis-with-r.html","id":"example-2-simulation-study","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.16.2 Example 2: Simulation Study","text":"","code":"\n# Function to simulate and analyze data\nsimulate_experiment <- function(n_samples, effect_size) {\n  # Generate data\n  control <- rnorm(n_samples, mean = 100, sd = 15)\n  treatment <- rnorm(n_samples, mean = 100 + effect_size, sd = 15)\n\n  # Perform t-test\n  test_result <- t.test(treatment, control)\n\n  # Return key results\n  list(\n    effect_size = effect_size,\n    p_value = test_result$p.value,\n    significant = test_result$p.value < 0.05\n  )\n}\n\n# Run simulations for different effect sizes\neffect_sizes <- seq(0, 10, by = 2)\nsimulation_results <- lapply(effect_sizes, function(es) {\n  simulate_experiment(n_samples = 50, effect_size = es)\n})\n\n# Convert to data frame for easy viewing\nresults_df <- do.call(rbind, lapply(simulation_results, as.data.frame))\nresults_df##   effect_size      p_value significant\n## 1           0 2.939728e-01       FALSE\n## 2           2 1.167613e-01       FALSE\n## 3           4 8.916570e-01       FALSE\n## 4           6 1.589971e-01       FALSE\n## 5           8 1.020368e-02        TRUE\n## 6          10 3.617470e-06        TRUE"},{"path":"tutorial-1-data-analysis-with-r.html","id":"summary","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.17 Summary","text":"tutorial covered essential advanced R programming concepts:Arrays: Multidimensional data structures complex dataLists: Flexible containers heterogeneous dataData Frames & Tibbles: Core structures statistical data analysisData Types: Understanding working different typesConditionals: Making decisions code -else ifelseFunctions: Creating reusable, modular codeLoops: loops iterationApply Functions: Functional programming lapply, sapply, vapplyThese tools form foundation efficient data analysis statistical programming R. Mastering enable write cleaner, maintainable, efficient R code.","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"key-takeaways","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.17.1 Key Takeaways","text":"Choose right data structure: vectors homogeneous data, lists heterogeneous, data frames tabular dataPrefer vectorized operations loops possibleWrite functions avoid repeating codeUse apply functions instead loops cleaner functional codeCheck handle missing values explicitlyUse type-safe functions (like vapply) production code","code":""},{"path":"tutorial-1-data-analysis-with-r.html","id":"further-learning","chapter":"2 Tutorial 1: Data Analysis with R","heading":"2.17.2 Further Learning","text":"continue developing R skills, explore:\n- Advanced data manipulation dplyr data.table\n- Functional programming purrr\n- Advanced visualization ggplot2\n- Writing efficient R code (vectorization, profiling)\n- Creating R packages reusable code","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"tutorial-2-simple-linear-regression","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3 Tutorial 2: Simple linear Regression","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"problem-1-simple-linear-regression-with-intercept","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1 Problem 1: Simple Linear Regression with Intercept","text":"study simple linear regression intercept:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\qquad = 1, \\ldots, n\n\\]regularity condition \\(\\sum_{=1}^n (X_i - \\bar{X})^2 > 0\\) (.e., \\(X_i\\) equal).Sample means:\\[\n\\bar{Y} = \\frac{1}{n} \\sum_{=1}^n Y_i, \\qquad \\bar{X} = \\frac{1}{n} \\sum_{=1}^n X_i\n\\]OLS objective: Minimize sum squared residuals:\\[\nS(\\beta_0, \\beta_1) = \\sum_{=1}^n \\left( Y_i - \\beta_0 - \\beta_1 X_i \\right)^2\n\\]Definitions:Fitted values: \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)Residuals: \\(\\hat{u}_i = Y_i - \\hat{Y}_i\\)","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"derive-the-ols-normal-equations","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.1 Derive the OLS Normal Equations","text":"Differentiate \\(S(\\beta_0, \\beta_1)\\) respect \\(\\beta_0\\) \\(\\beta_1\\) set equal zero.First-Order Condition \\(\\beta_0\\):\\[\n\\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{=1}^n \\left( Y_i - \\beta_0 - \\beta_1 X_i \\right) = 0\n\\]simplifies :\\[\n\\sum_{=1}^n \\left( Y_i - \\beta_0 - \\beta_1 X_i \\right) = 0\n\\]Expanding:\\[\n\\sum_{=1}^n Y_i = n \\beta_0 + \\beta_1 \\sum_{=1}^n X_i\n\\]Solving \\(\\beta_0\\):\\[\n\\boxed{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}}\n\\]First-Order Condition \\(\\beta_1\\):\\[\n\\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{=1}^n X_i \\left( Y_i - \\beta_0 - \\beta_1 X_i \\right) = 0\n\\]simplifies :\\[\n\\sum_{=1}^n X_i \\left( Y_i - \\beta_0 - \\beta_1 X_i \\right) = 0\n\\]Derive Slope Centered Form:Substitute \\(\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\\) residual expression:\\[\nY_i - \\beta_0 - \\beta_1 X_i = Y_i - (\\bar{Y} - \\beta_1 \\bar{X}) - \\beta_1 X_i = (Y_i - \\bar{Y}) - \\beta_1 (X_i - \\bar{X})\n\\]Minimizing \\(\\sum_{=1}^n \\left[ (Y_i - \\bar{Y}) - \\beta_1 (X_i - \\bar{X}) \\right]^2\\) respect \\(\\beta_1\\) gives:\\[\n\\sum_{=1}^n (X_i - \\bar{X}) \\left[ (Y_i - \\bar{Y}) - \\beta_1 (X_i - \\bar{X}) \\right] = 0\n\\]Rearranging:\\[\n\\sum_{=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\beta_1 \\sum_{=1}^n (X_i - \\bar{X})^2\n\\]OLS Estimators:\\[\n\\boxed{\\hat{\\beta}_1 = \\frac{\\sum_{=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{=1}^n (X_i - \\bar{X})^2}}\n\\]\\[\n\\boxed{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residual-orthogonality-properties","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2 Residual Orthogonality Properties","text":"key properties follow directly first-order conditions.","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residuals-sum-to-zero","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.1 Residuals Sum to Zero","text":"FOC \\(\\beta_0\\) evaluated \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\):\\[\n\\sum_{=1}^n \\left( Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i \\right) = 0\n\\]Therefore:\\[\n\\boxed{\\sum_{=1}^n \\hat{u}_i = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residuals-are-orthogonal-to-x","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.2 Residuals are Orthogonal to \\(X\\)","text":"FOC \\(\\beta_1\\) evaluated \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\):\\[\n\\sum_{=1}^n X_i \\left( Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i \\right) = 0\n\\]Therefore:\\[\n\\boxed{\\sum_{=1}^n X_i \\hat{u}_i = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residuals-are-orthogonal-to-x_i---barx","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.3 Residuals are Orthogonal to \\((X_i - \\bar{X})\\)","text":"Using result \\(\\sum_{=1}^n \\hat{u}_i = 0\\):\\[\n\\sum_{=1}^n (X_i - \\bar{X}) \\hat{u}_i = \\sum_{=1}^n X_i \\hat{u}_i - \\bar{X} \\sum_{=1}^n \\hat{u}_i = 0 - \\bar{X} \\cdot 0 = 0\n\\]Therefore:\\[\n\\boxed{\\sum_{=1}^n (X_i - \\bar{X}) \\hat{u}_i = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residuals-are-orthogonal-to-fitted-values","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.4 Residuals are Orthogonal to fitted values","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"question","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.5 Question","text":"Show :\\[\n\\boxed{\\sum_{=1}^n \\hat Y_i \\hat u_i = 0.}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"solution","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.2.6 Solution","text":"Write fitted values \\(\\hat Y_i=\\hat\\beta_0+\\hat\\beta_1X_i\\) expand:\\[\n\\sum_{=1}^n \\hat Y_i\\hat u_i\n=\n\\sum_{=1}^n (\\hat\\beta_0+\\hat\\beta_1X_i)\\hat u_i\n=\n\\hat\\beta_0\\sum_{=1}^n \\hat u_i + \\hat\\beta_1\\sum_{=1}^n X_i\\hat u_i.\n\\]A1, sums zero. Therefore:\\[\n\\boxed{\\sum_{=1}^n \\hat Y_i \\hat u_i = 0.}\n\\]Interpretation. part OLS explains (\\(\\hat Y\\)) part explain (\\(\\hat u\\)) move together sample.","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"properties-of-fitted-values","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.3 Properties of Fitted Values","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"the-regression-line-passes-through-barx-bary","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.3.1 The Regression Line Passes Through \\((\\bar{X}, \\bar{Y})\\)","text":"Taking average fitted values:\\[\n\\overline{\\hat{Y}} = \\frac{1}{n} \\sum_{=1}^n \\hat{Y}_i = \\frac{1}{n} \\sum_{=1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X}\n\\]Since \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\\):\\[\n\\overline{\\hat{Y}} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X}) + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]Equivalently, \\(X = \\bar{X}\\):\\[\n\\hat{Y}(\\bar{X}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\]Conclusion: OLS fitted line passes point \\((\\bar{X}, \\bar{Y})\\).","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"mean-residual-is-zero","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.3.2 Mean Residual is Zero","text":"Since \\(\\sum_{=1}^n \\hat{u}_i = 0\\):\\[\n\\boxed{\\bar{\\hat{u}} = \\frac{1}{n} \\sum_{=1}^n \\hat{u}_i = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"decomposition-of-variation-tss-ess-rss","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.4 Decomposition of Variation (TSS = ESS + RSS)","text":"Definitions:","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"decomposition","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.4.1 Decomposition","text":"Start :\\[\nY_i - \\bar{Y} = (\\hat{Y}_i - \\bar{Y}) + (Y_i - \\hat{Y}_i) = (\\hat{Y}_i - \\bar{Y}) + \\hat{u}_i\n\\]Square sides sum:\\[\n\\sum_{=1}^n (Y_i - \\bar{Y})^2 = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y})^2 + \\sum_{=1}^n \\hat{u}_i^2 + 2 \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y}) \\hat{u}_i\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"the-cross-term-vanishes","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.4.2 The Cross-Term Vanishes","text":"Note :\\[\n\\hat{Y}_i - \\bar{Y} = (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) - \\bar{Y} = (\\bar{Y} - \\hat{\\beta}_1 \\bar{X} + \\hat{\\beta}_1 X_i) - \\bar{Y} = \\hat{\\beta}_1 (X_i - \\bar{X})\n\\]Therefore:\\[\n\\sum_{=1}^n (\\hat{Y}_i - \\bar{Y}) \\hat{u}_i = \\hat{\\beta}_1 \\sum_{=1}^n (X_i - \\bar{X}) \\hat{u}_i = \\hat{\\beta}_1 \\cdot 0 = 0\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"result","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.4.3 Result","text":"\\[\n\\boxed{\\text{TSS} = \\text{ESS} + \\text{RSS}}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"sample-covariance-implications","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.5 Sample Covariance Implications","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"sample-covariance-between-x-and-hatu-is-zero","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.5.1 Sample Covariance Between \\(X\\) and \\(\\hat{u}\\) is Zero","text":"Since \\(\\sum_{=1}^n \\hat{u}_i = 0\\), sample covariance \\(X\\) \\(\\hat{u}\\) proportional :\\[\n\\sum_{=1}^n (X_i - \\bar{X}) \\hat{u}_i = 0\n\\]Therefore:\\[\n\\boxed{\\widehat{\\text{Cov}}(X, \\hat{u}) = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"fitted-values-are-orthogonal-to-residuals","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.6 Fitted Values are Orthogonal to Residuals","text":"\\[\n\\sum_{=1}^n \\hat{Y}_i \\hat{u}_i = \\sum_{=1}^n (\\hat{Y}_i - \\bar{Y}) \\hat{u}_i + \\bar{Y} \\sum_{=1}^n \\hat{u}_i = 0 + \\bar{Y} \\cdot 0 = 0\n\\]Therefore:\\[\n\\boxed{\\sum_{=1}^n \\hat{Y}_i \\hat{u}_i = 0}\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"summary-of-key-results","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.7 Summary of Key Results","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"ols-estimators","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.7.1 OLS Estimators","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"residual-properties","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.7.2 Residual Properties","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"additional-properties","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.1.7.3 Additional Properties","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"problem-2-wooldridge-computer-exercise-c1-401k-participation-and-match-rate","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2 Problem 2: Wooldridge Computer Exercise C1 (401K): Participation and Match Rate","text":"","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"overview","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.1 Overview","text":"notebook reproduces Wooldridge, Computer Exercise C1 (401K). Goal. Use plan-level data study whether generous employer match rate associated higher 401(k) participation.Outcome: prate = percentage eligible workers active 401(k) account.Regressor: mrate = match rate (average firm contribution per $1 worker contribution).estimate simple linear regression:\n\\[\nprate = \\beta_0 + \\beta_1 \\, mrate + u.\n\\]","code":""},{"path":"tutorial-2-simple-linear-regression.html","id":"load-packages-and-data","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.2 Load packages and data","text":"","code":"\n# If you do not have the package installed, uncomment the next line:\ninstall.packages(\"wooldridge\")## # Downloading packages -------------------------------------------------------\n## - Downloading wooldridge from CRAN ...          OK [4 Mb in 2.4s]\n## Successfully downloaded 1 package in 2.5 seconds.\n## \n## The following package(s) will be installed:\n## - wooldridge [1.4-4]\n## These packages will be installed into \"~/Library/CloudStorage/Dropbox/Nicolas/Cursos/01 Libro - análisis de datos/renv/library/R-4.2/aarch64-apple-darwin20\".\n## \n## \n## # Installing packages --------------------------------------------------------\n## - Installing wooldridge ...                     OK [installed binary and cached]\n## Successfully installed 1 package in 0.14 seconds.\nlibrary(wooldridge)\n\n# Load the dataset used in this exercise.\ndata(\"k401k\")\ndf <- k401k\n\n# Quick check: dimensions and variable names\ndim(df)## [1] 1534    8\nnames(df)## [1] \"prate\"   \"mrate\"   \"totpart\" \"totelg\"  \"age\"     \"totemp\"  \"sole\"    \"ltotemp\""},{"path":"tutorial-2-simple-linear-regression.html","id":"i-compute-the-sample-averages-of-participation-and-match-rates","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.3 (i) Compute the sample averages of participation and match rates","text":"Interpretation.\n- mean_prate average percentage eligible workers participating across plans.\n- mean_mrate average employer match rate across plans.","code":"\n# In this dataset:\n# - prate is the plan participation rate (in percentage points)\n# - mrate is the match rate\n\nmean_prate <- mean(df$prate, na.rm = TRUE)\nmean_mrate <- mean(df$mrate, na.rm = TRUE)\n\nmean_prate## [1] 87.36291\nmean_mrate## [1] 0.7315124"},{"path":"tutorial-2-simple-linear-regression.html","id":"ii-estimate-the-simple-regression-prate-on-mrate","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.4 (ii) Estimate the simple regression: prate on mrate","text":"","code":"\n# Estimate the simple OLS regression model\nm1 <- lm(prate ~ mrate, data = df)\n\n# Summary includes coefficient estimates, standard errors, t-stats, and R^2\ns1 <- summary(m1)\ns1## \n## Call:\n## lm(formula = prate ~ mrate, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -82.303  -8.184   5.178  12.712  16.807 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  83.0755     0.5633  147.48   <2e-16 ***\n## mrate         5.8611     0.5270   11.12   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.09 on 1532 degrees of freedom\n## Multiple R-squared:  0.0747, Adjusted R-squared:  0.0741 \n## F-statistic: 123.7 on 1 and 1532 DF,  p-value: < 2.2e-16\n# Report the sample size used by the regression (after dropping any missing values)\nn <- nobs(m1)\n\n# Extract R-squared (fraction of sample variation in prate explained by mrate)\nr2 <- s1$r.squared\n\nn## [1] 1534\nr2## [1] 0.0747031"},{"path":"tutorial-2-simple-linear-regression.html","id":"iii-interpret-the-intercept-and-the-slope","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.5 (iii) Interpret the intercept and the slope","text":"read coefficients (economic meaning).Intercept (\\(\\hat\\beta_0\\)): Predicted participation rate mrate = 0.\nfitted prate plan employer match.\nNote: Interpretation meaningful mrate = 0 (near) support data.\nfitted prate plan employer match.Note: Interpretation meaningful mrate = 0 (near) support data.Slope (\\(\\hat\\beta_1\\)): Predicted change participation (percentage points) one-unit increase mrate.\nSince mrate measures many dollars firm contributes per $1 worker contributes,\none-unit change economically large (e.g., 0.5 1.5).\nPractically, may also interpret smaller changes:\n0.10 increase mrate changes predicted participation \\(0.10 \\times \\hat\\beta_1\\).\nSince mrate measures many dollars firm contributes per $1 worker contributes,\none-unit change economically large (e.g., 0.5 1.5).Practically, may also interpret smaller changes:\n0.10 increase mrate changes predicted participation \\(0.10 \\times \\hat\\beta_1\\).","code":"\n# Extract coefficients\nb0 <- coef(m1)[1]  # intercept\nb1 <- coef(m1)[2]  # slope on mrate\n\nb0## (Intercept) \n##    83.07546\nb1##    mrate \n## 5.861079\n# Example: predicted change in prate for a 0.10 increase in mrate\ndelta_mrate <- 0.10\npred_change_prate <- delta_mrate * b1\npred_change_prate##     mrate \n## 0.5861079"},{"path":"tutorial-2-simple-linear-regression.html","id":"iv-predicted-participation-when-mrate-3.5.-is-it-reasonable","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.6 (iv) Predicted participation when mrate = 3.5. Is it reasonable?","text":"assess whether prediction reasonable, check whether mrate = 3.5 lies within observed range data.\nPredictions far outside support mrate extrapolations, can unreliable.Discussion prompt (happening looks unreasonable):\n- 3.5 much larger typical match rates data, fitted value based \nextending linear trend beyond information.\n- addition, prate percentage generally lie 0 100; linear model can\nproduce fitted values outside range, especially extrapolation.","code":"\n# Prediction at mrate = 3.5\npred_35 <- predict(m1, newdata = data.frame(mrate = 3.5))\npred_35##        1 \n## 103.5892\n# Range of mrate in the sample\nrange_mrate <- range(df$mrate, na.rm = TRUE)\nrange_mrate## [1] 0.01 4.91\n# Also helpful: a few quantiles to understand typical values\nquantile(df$mrate, probs = c(0, .05, .25, .5, .75, .95, 1), na.rm = TRUE)##     0%     5%    25%    50%    75%    95%   100% \n## 0.0100 0.1100 0.3000 0.4600 0.8300 2.3635 4.9100"},{"path":"tutorial-2-simple-linear-regression.html","id":"v-how-much-of-the-variation-in-prate-is-explained-by-mrate","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.7 (v) How much of the variation in prate is explained by mrate?","text":"share variation prate explained mrate simple model R-squared:Interpretation.\n- \\(R^2\\) fraction sample variation participation accounted variation match rate.\n- Whether “lot” depends context; cross-sectional data, modest \\(R^2\\) values common.","code":"\nr2## [1] 0.0747031"},{"path":"tutorial-2-simple-linear-regression.html","id":"optional-quick-plot","chapter":"3 Tutorial 2: Simple linear Regression","heading":"3.2.8 Optional: quick plot","text":"","code":"\n# A simple scatter plot with the fitted regression line.\nplot(df$mrate, df$prate,\n     xlab = \"Match rate (mrate)\",\n     ylab = \"Participation rate (prate)\",\n     main = \"401(k) Participation vs Match Rate\")\nabline(m1, lwd = 2)"},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","text":"observe ..d. sample \\(\\{(Y_i, X_i)\\}_{=1}^n\\) \n\\(\\sum_{=1}^n (X_i-\\bar X)^2>0\\). simple linear regression model \\[\nY = \\beta_0 + \\beta_1 X + u.\n\\]sample, OLS chooses \\((\\hat\\beta_0,\\hat\\beta_1)\\) minimize sum squared residuals:\\[\n\\min_{\\beta_0,\\beta_1}\\ \\sum_{=1}^n (Y_i-\\beta_0-\\beta_1X_i)^2.\n\\]Define fitted values residuals:\\[\n\\hat Y_i = \\hat\\beta_0+\\hat\\beta_1X_i,\n\\qquad\n\\hat u_i = Y_i-\\hat Y_i.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"part-a.-what-residuals-are-and-what-ols-forces-them-to-satisfy","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1 Part A. What residuals are and what OLS forces them to satisfy","text":"Narrative idea. OLS picks “best” line (squared-error sense). line chosen, residual \\(\\hat u_i\\) part \\(Y_i\\) line explain. key point : OLS leave residuals arbitrary. first-order conditions imply exact sample moment conditions—mechanical identities hold dataset whenever run OLS intercept.use normal equations facts (derived last tutorial), focus imply.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"a1.-normal-equations-two-sample-moment-conditions","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.1 A1. Normal equations: two sample moment conditions","text":"OLS normal equations imply:\\[\n\\boxed{\\sum_{=1}^n \\hat u_i = 0}\n\\qquad\\text{}\\qquad\n\\boxed{\\sum_{=1}^n X_i \\hat u_i = 0.}\n\\]Interpretation.\n- \\(\\sum \\hat u_i=0\\) means residuals average zero: OLS systematically - -predict \\(Y\\) sample.\n- \\(\\sum X_i\\hat u_i=0\\) means residuals “orthogonal” \\(X\\) sample: slope chosen, remaining linear association \\(X\\) residuals.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"a2.-orthogonality-to-centered-x-what-it-really-means","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.2 A2. Orthogonality to centered \\(X\\): what it really means","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-1","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.2.1 Question","text":"Show residuals also orthogonal deviations \\(X\\) around mean:\\[\n\\boxed{\\sum_{=1}^n (X_i-\\bar X)\\hat u_i = 0.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-1","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.2.2 Solution","text":"Start identity:\\[\n\\sum_{=1}^n (X_i-\\bar X)\\hat u_i\n=\n\\sum_{=1}^n X_i\\hat u_i\n-\n\\bar X\\sum_{=1}^n \\hat u_i.\n\\]normal equations (A1), \\(\\sum X_i\\hat u_i=0\\) \\(\\sum \\hat u_i=0\\). Therefore right-hand side \\(0-\\bar X\\cdot 0=0\\), :\\[\n\\boxed{\\sum_{=1}^n (X_i-\\bar X)\\hat u_i = 0.}\n\\]Interpretation. fitting line, observations -average \\(X\\) systematically /line relative observations -average \\(X\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"a3-prove-that-the-ols-regression-line-passes-through-barx-bary","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.3 A3 Prove that the OLS regression line passes through \\((\\bar{X}, \\bar{Y})\\)","text":"Step 1: Write fitted value equationFor observation \\(\\), fitted value :\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n\\]Step 2: Compute mean fitted valuesSum \\(\\hat{Y}_i\\) divide \\(n\\):\\[\n\\bar{\\hat{Y}} = \\frac{1}{n} \\sum_{=1}^{n} \\hat{Y}_i\n= \\frac{1}{n} \\sum_{=1}^{n} \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i \\right)\n\\]Step 3: Split summation\\[\n\\bar{\\hat{Y}} = \\frac{1}{n} \\sum_{=1}^{n} \\hat{\\beta}_0 + \\frac{1}{n} \\sum_{=1}^{n} \\hat{\\beta}_1 X_i\n\\]Since \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) constants, factor :\\[\n\\bar{\\hat{Y}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\frac{1}{n} \\sum_{=1}^{n} X_i\n= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X}\n\\]Step 4: Substitute \\(\\hat{\\beta}_0\\)Recall OLS intercept estimator :\\[\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\n\\]Substituting:\\[\n\\bar{\\hat{Y}} = \\left( \\bar{Y} - \\hat{\\beta}_1 \\bar{X} \\right) + \\hat{\\beta}_1 \\bar{X}\n\\]Step 5: SimplifyThe terms \\(-\\hat{\\beta}_1 \\bar{X}\\) \\(+\\hat{\\beta}_1 \\bar{X}\\) cancel :\\[\n\\bar{\\hat{Y}} = \\bar{Y}\n\\]Step 6: Geometric interpretationEvaluating fitted line \\(X = \\bar{X}\\):\\[\n\\hat{Y}(\\bar{X}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{X} = \\bar{Y}\n\\], \\(X\\) equals mean, line predicts exactly \\(\\bar{Y}\\).Conclusion: OLS regression line always passes point \\((\\bar{X}, \\bar{Y})\\). \\(\\blacksquare\\)","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"a4.-prove-the-anova-decomposition-tss-ess-rss","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4 A4. Prove the ANOVA decomposition: \\(TSS = ESS + RSS\\)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"context","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4.1 Context","text":"regression analysis want know much total variation \\(Y\\) explained model. answer , decompose total variation two parts: one attributed fitted line one residuals. decomposition foundation \\(R^2\\) statistic \\(F\\)-test regression.define:\\[\nTSS=\\sum_{=1}^n (Y_i-\\bar Y)^2,\\quad\nESS=\\sum_{=1}^n (\\hat Y_i-\\bar Y)^2,\\quad\nRSS=\\sum_{=1}^n \\hat u_i^2.\n\\]TSS (Total Sum Squares): measures total variability \\(Y\\) around mean.ESS (Explained Sum Squares): measures much variability captured fitted values \\(\\hat{Y}_i\\).RSS (Residual Sum Squares): measures leftover variability captured model.key identity start :\\[\nY_i - \\bar{Y} = (\\hat{Y}_i - \\bar{Y}) + \\hat{u}_i\n\\]simply says: deviation observation mean equals deviation explained model plus residual.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-2","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4.2 Question","text":"() Square sides identity sum \\(= 1, \\dots, n\\). Write result terms \\(TSS\\), \\(ESS\\), \\(RSS\\), cross-term.(b) Using result A2 (\\(\\sum (X_i - \\bar{X})\\hat{u}_i = 0\\)) A3 (\\(\\hat{Y}_i - \\bar{Y} = \\hat{\\beta}_1(X_i - \\bar{X})\\)), show cross-term equals zero conclude:\\[\n\\boxed{TSS = ESS + RSS.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-2","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4.3 Solution","text":"() Squaring summing:\\[\n\\sum_{=1}^n (Y_i-\\bar Y)^2\n=\n\\sum_{=1}^n (\\hat Y_i-\\bar Y)^2\n+\n\\sum_{=1}^n \\hat u_i^2\n+\n2\\sum_{=1}^n (\\hat Y_i-\\bar Y)\\hat u_i\n\\]:\\[\nTSS = ESS + RSS + 2\\sum_{=1}^n (\\hat Y_i-\\bar Y)\\hat u_i\n\\](b) need show cross-term zero.Step 1. A3, regression line passes \\((\\bar{X}, \\bar{Y})\\), :\\[\n\\hat Y_i - \\bar Y\n= (\\hat\\beta_0 + \\hat\\beta_1 X_i) - (\\hat\\beta_0 + \\hat\\beta_1 \\bar X)\n= \\hat\\beta_1(X_i - \\bar X)\n\\]Step 2. Substitute cross-term:\\[\n\\sum_{=1}^n (\\hat Y_i - \\bar Y)\\hat u_i\n= \\hat\\beta_1 \\sum_{=1}^n (X_i - \\bar X)\\hat u_i\n\\]Step 3. result A2, \\(\\sum_{=1}^n (X_i - \\bar X)\\hat u_i = 0\\), entire cross-term vanishes.Step 4. Substituting back:\\[\nTSS = ESS + RSS + 2 \\cdot 0 = ESS + RSS\n\\]\\[\n\\boxed{TSS = ESS + RSS}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"interpretation","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4.4 Interpretation","text":"result tells us total variation \\(Y\\) can cleanly split two non-overlapping parts: model explains (\\(ESS\\)) (\\(RSS\\)). clean split makes coefficient determination meaningful:\\[\nR^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]Without cross-term zero, decomposition hold, \\(R^2\\) lose interpretation proportion variance explained.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"computational-check-in-r","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.1.4.5 Computational check in R","text":"chunk verifies ANOVA decomposition numerically simulated dataset.values essentially zero (floating-point precision), confirming decomposition.","code":"\nset.seed(123)\n\nn <- 80\nX <- rnorm(n, mean = 2, sd = 1)\nu <- rnorm(n, mean = 0, sd = 2)\nY <- 1 + 1.5 * X + u\n\nfit <- lm(Y ~ X)\nuhat <- resid(fit)\nyhat <- fitted(fit)\n\nTSS <- sum((Y - mean(Y))^2)\nESS <- sum((yhat - mean(Y))^2)\nRSS <- sum(uhat^2)\n\nc(\n  cross_term        = sum((yhat - mean(Y)) * uhat),\n  TSS_minus_ESS_RSS = TSS - ESS - RSS\n)##        cross_term TSS_minus_ESS_RSS \n##     -6.494805e-15      0.000000e+00"},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"part-b.-the-population-regression-function-prf","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2 Part B. The Population Regression Function (PRF)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"why-do-we-need-this","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.1 Why do we need this?","text":"Part worked entirely sample data: found formulas \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), proved algebraic properties residuals, showed ANOVA decomposition \\(TSS = ESS + RSS\\). purely mechanical — holds dataset, assumptions data came .ultimate goal regression just draw line particular sample. want learn something underlying population. raises fundamental questions:exactly OLS trying estimate?conditions sample estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) actually tell us something true world?can trust estimates, might mislead us?answer questions, need define population counterpart sample regression — Population Regression Function. Part B builds theoretical framework allow us, later sections, prove OLS unbiased understand can fail.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b1.a-definition-concept","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.2 B1.a Definition (concept)","text":"Population Regression Function :\\[\nm(x) \\equiv \\mathbb{E}[Y \\mid X = x].\n\\]Interpretation. value \\(x\\), \\(m(x)\\) average \\(Y\\) among units population \\(X = x\\). “true” relationship \\(X\\) \\(Y\\) — signal trying recover noisy data.many applications approximate conditional expectation linear function:\\[\n\\mathbb{E}[Y \\mid X = x] = \\beta_0 + \\beta_1 x.\n\\]linear PRF assumption. \\(\\beta_0\\) \\(\\beta_1\\) fixed, unknown population parameters — quantities sample estimates \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) Part trying approximate.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b2.-zero-conditional-mean-what-it-is-and-where-it-comes-from","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.3 B2. Zero conditional mean: what it is and where it comes from","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"context-1","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.3.1 Context","text":"Part defined residuals \\(\\hat{u}_i = Y_i - \\hat{Y}_i\\) showed satisfy convenient algebraic properties (summing zero, orthogonal \\(X\\)). sample properties hold construction.Now ask: population error term \\(u\\) satisfy analogous properties? answer yes — construction. follows linear PRF assumption. result, called zero conditional mean condition, single important assumption OLS theory: makes estimators unbiased.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-3","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.3.2 Question","text":"Assume PRF linear:\\[\n\\mathbb{E}[Y \\mid X = x] = \\beta_0 + \\beta_1 x.\n\\]Define population error term:\\[\nu \\equiv Y - (\\beta_0 + \\beta_1 X).\n\\]Show linear PRF implies:\\[\n\\boxed{\\mathbb{E}[u \\mid X] = 0.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-3","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.3.3 Solution","text":"Step 1. Apply conditional expectation definition \\(u\\):\\[\n\\mathbb{E}[u \\mid X]\n= \\mathbb{E}[Y - (\\beta_0 + \\beta_1 X) \\mid X]\n= \\mathbb{E}[Y \\mid X] - (\\beta_0 + \\beta_1 X)\n\\]Step 2. Substitute linear PRF, \\(\\mathbb{E}[Y \\mid X] = \\beta_0 + \\beta_1 X\\):\\[\n\\mathbb{E}[u \\mid X] = (\\beta_0 + \\beta_1 X) - (\\beta_0 + \\beta_1 X) = 0\n\\]\\[\n\\boxed{\\mathbb{E}[u \\mid X] = 0}\n\\]Interpretation. accounting linear effect \\(X\\), remaining component \\(u\\) systematic pattern left — average, zero regardless value \\(X\\). Compare Part , showed \\(\\sum \\hat{u}_i = 0\\) \\(\\sum X_i \\hat{u}_i = 0\\) algebra. , analogous population property holds PRF assumption, construction.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b3.-what-zero-conditional-mean-implies-useful-corollaries","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.4 B3. What zero conditional mean implies (useful corollaries)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"context-2","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.4.1 Context","text":"condition \\(\\mathbb{E}[u \\mid X] = 0\\) conditional statement — says something \\(u\\) every possible value \\(X\\). strong requirement, automatically implies weaker unconditional properties. unconditional properties connect back moment conditions saw Part (recall: \\(\\sum \\hat{u}_i = 0\\) \\(\\sum X_i \\hat{u}_i = 0\\) sample analogues).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-4","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.4.2 Question","text":"Assuming \\(\\mathbb{E}[u \\mid X] = 0\\), show :\\(\\boxed{\\mathbb{E}[u] = 0}\\)\\(\\boxed{\\operatorname{Cov}(X, u) = 0}\\)","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-4","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.4.3 Solution","text":"1) law iterated expectations:\\[\n\\mathbb{E}[u] = \\mathbb{E}\\big[\\mathbb{E}[u \\mid X]\\big] = \\mathbb{E}[0] = 0\n\\]2) Start definition:\\[\n\\operatorname{Cov}(X, u) = \\mathbb{E}[Xu] - \\mathbb{E}[X]\\,\\mathbb{E}[u]\n\\]Since \\(\\mathbb{E}[u] = 0\\) part 1, suffices show \\(\\mathbb{E}[Xu] = 0\\):\\[\n\\mathbb{E}[Xu]\n= \\mathbb{E}\\big[\\mathbb{E}[Xu \\mid X]\\big]\n= \\mathbb{E}\\big[X \\cdot \\mathbb{E}[u \\mid X]\\big]\n= \\mathbb{E}[X \\cdot 0] = 0\n\\]Therefore \\(\\operatorname{Cov}(X, u) = 0\\).Interpretation. population analogues Part results:sample properties hold automatically OLS fit. population properties require zero conditional mean assumption. parallel coincidence — OLS designed sample moment conditions mimic population ones.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b4.-mean-independence-vs-zero-conditional-mean-and-why-we-care","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.5 B4. Mean independence vs zero conditional mean (and why we care)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"context-3","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.5.1 Context","text":"Students sometimes encounter different versions “relationship \\(u\\) \\(X\\)” assumption. clarify two common ones logical relationship.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b4.a-definitions","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.5.2 B4.a Definitions","text":"Zero conditional mean:\n\\[\n\\mathbb{E}[u \\mid X] = 0\n\\]Zero conditional mean:\n\\[\n\\mathbb{E}[u \\mid X] = 0\n\\]Mean independence:\n\\[\n\\mathbb{E}[u \\mid X] = \\mathbb{E}[u]\n\\]Mean independence:\n\\[\n\\mathbb{E}[u \\mid X] = \\mathbb{E}[u]\n\\]Mean independence says conditional mean \\(u\\) depend \\(X\\) ; zero conditional mean additionally pins constant zero.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-5","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.5.3 Question","text":"\\(\\mathbb{E}[u] = 0\\), show mean independence implies zero conditional mean.2 lines: stronger, ?","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-5","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.5.4 Solution","text":"1) mean independence holds, \\(\\mathbb{E}[u \\mid X] = \\mathbb{E}[u]\\). also \\(\\mathbb{E}[u] = 0\\):\\[\n\\mathbb{E}[u \\mid X] = 0\n\\]2) Mean independence stronger requires \\(\\mathbb{E}[u \\mid X]\\) constant every value \\(X\\). Zero conditional mean special case constant zero — specific condition needed OLS unbiasedness.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"b5.-a-cautionary-note-uncorrelatedness-is-not-enough-concept","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.6 B5. A cautionary note: uncorrelatedness is not enough (concept)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"context-4","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.6.1 Context","text":"B3, know \\(\\mathbb{E}[u \\mid X] = 0\\) implies \\(\\operatorname{Cov}(X, u) = 0\\). tempting think reverse also true — \\(X\\) \\(u\\) uncorrelated, OLS must fine. common dangerous misconception.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-2-lines","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.6.2 Question (2 lines)","text":"Explain briefly \\(\\operatorname{Cov}(X, u) = 0\\) alone guarantee \\(\\mathbb{E}[u \\mid X] = 0\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-6","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.6.3 Solution","text":"Uncorrelatedness unconditional moment restriction — rules linear relationship \\(X\\) \\(u\\). can hold even \\(\\mathbb{E}[u \\mid X]\\) varies \\(X\\) nonlinear way (e.g., \\(\\mathbb{E}[u \\mid X] = X^2 - \\mathbb{E}[X^2]\\)). OLS unbiasedness requires full conditional restriction \\(\\mathbb{E}[u \\mid X] = 0\\), merely \\(\\operatorname{Cov}(X, u) = 0\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"quick-simulation-intuition-in-r","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.2.7 Quick simulation intuition in R","text":"chunk just build intuition: shows generate data \\(\\mathbb{E}[u\\mid X]=0\\), sample correlation \\(X\\) residual-like noise fluctuates around zero.","code":"\nset.seed(123)\n\nn <- 500\nX <- rnorm(n)\nu <- rnorm(n)                 # independent of X => E[u|X]=0\nY <- 1 + 2*X + u\n\n# Check sample correlation between X and u (should be close to 0 on average)\ncor(X, u)## [1] -0.05193691"},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"part-c.-unbiasedness-of-ols-with-solutions","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3 Part C. Unbiasedness of OLS (with solutions)","text":"Narrative idea. Part gave sample identities (orthogonality) hold mechanically.\nPart B defined population object care (PRF) introduced key assumption \\(\\mathbb{E}[u\\mid X]=0\\).\nPart C connects two: use algebraic representation OLS slope show random sampling zero conditional mean, OLS unbiased.work population model (observation \\(\\)):\\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i.\n\\]Assume ..d. sampling zero conditional mean assumption:\\[\n\\mathbb{E}[u_i\\mid X_i] = 0.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"c1.-key-representation-of-the-ols-slope","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.1 C1. Key representation of the OLS slope","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-6","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.1.1 Question","text":"Using known formula OLS slope,\\[\n\\hat\\beta_1=\\frac{\\sum_{=1}^n (X_i-\\bar X)(Y_i-\\bar Y)}{\\sum_{=1}^n (X_i-\\bar X)^2},\n\\]show :\\[\n\\boxed{\\hat\\beta_1\n=\n\\beta_1\n+\n\\frac{\\sum_{=1}^n (X_i-\\bar X)u_i}{\\sum_{=1}^n (X_i-\\bar X)^2}.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-7","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.1.2 Solution","text":"Start decomposition:\\[\nY_i-\\bar Y\n=\n\\beta_1(X_i-\\bar X) + (u_i-\\bar u),\n\\]\\(\\bar Y=\\beta_0+\\beta_1\\bar X+\\bar u\\).Multiply sides \\((X_i-\\bar X)\\) sum \\(\\):\\[\n\\sum (X_i-\\bar X)(Y_i-\\bar Y)\n=\n\\beta_1\\sum (X_i-\\bar X)^2 + \\sum (X_i-\\bar X)(u_i-\\bar u).\n\\]\\(\\sum (X_i-\\bar X)\\bar u = \\bar u\\sum (X_i-\\bar X)=0\\), \\[\n\\sum (X_i-\\bar X)(u_i-\\bar u)=\\sum (X_i-\\bar X)u_i.\n\\]Divide sides \\(S_{xx}=\\sum (X_i-\\bar X)^2\\):\\[\n\\hat\\beta_1\n=\n\\beta_1 + \\frac{\\sum (X_i-\\bar X)u_i}{S_{xx}}.\n\\]Hence desired representation holds.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"c2.-unbiasedness-of-the-slope-conditional-then-unconditional","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.2 C2. Unbiasedness of the slope: conditional then unconditional","text":"Narrative idea. Unbiasedness statement repeated sampling. first show unbiasedness conditional observed \\(X\\) sample, take expectations get unconditional unbiasedness.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-7","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.2.1 Question","text":"Show:\\[\n\\boxed{\\mathbb{E}[\\hat\\beta_1\\mid X_1,\\dots,X_n]=\\beta_1}\n\\quad\\Rightarrow\\quad\n\\boxed{\\mathbb{E}[\\hat\\beta_1]=\\beta_1.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-8","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.2.2 Solution","text":"C1:\\[\n\\hat\\beta_1-\\beta_1\n=\n\\frac{1}{S_{xx}}\\sum_{=1}^n (X_i-\\bar X)u_i,\n\\qquad\nS_{xx}=\\sum (X_i-\\bar X)^2.\n\\]Condition \\(X=(X_1,\\dots,X_n)\\). weights \\((X_i-\\bar X)/S_{xx}\\) constants given \\(X\\), \\[\n\\mathbb{E}[\\hat\\beta_1-\\beta_1\\mid X]\n=\n\\frac{1}{S_{xx}}\\sum (X_i-\\bar X)\\,\\mathbb{E}[u_i\\mid X].\n\\]..d. sampling \\(\\mathbb{E}[u_i\\mid X_i]=0\\), \\(\\mathbb{E}[u_i\\mid X]=0\\) \\(\\). Therefore\\[\n\\mathbb{E}[\\hat\\beta_1-\\beta_1\\mid X]=0\n\\quad\\Rightarrow\\quad\n\\boxed{\\mathbb{E}[\\hat\\beta_1\\mid X]=\\beta_1.}\n\\]Finally, apply Law Iterated Expectations:\\[\n\\mathbb{E}[\\hat\\beta_1]\n=\n\\mathbb{E}[\\mathbb{E}[\\hat\\beta_1\\mid X]]\n=\n\\mathbb{E}[\\beta_1]=\\beta_1.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"c3.-unbiasedness-of-the-intercept","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.3 C3. Unbiasedness of the intercept","text":"Narrative idea. unbiasedness slope, unbiasedness intercept follows identity \\(\\hat\\beta_0=\\bar Y-\\hat\\beta_1\\bar X\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-8","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.3.1 Question","text":"Show:\\[\n\\boxed{\\mathbb{E}[\\hat\\beta_0\\mid X_1,\\dots,X_n]=\\beta_0}\n\\quad\\Rightarrow\\quad\n\\boxed{\\mathbb{E}[\\hat\\beta_0]=\\beta_0.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-9","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.3.2 Solution","text":"Start :\\[\n\\hat\\beta_0 = \\bar Y - \\hat\\beta_1\\bar X.\n\\]Using \\(\\bar Y=\\beta_0+\\beta_1\\bar X+\\bar u\\),\\[\n\\hat\\beta_0-\\beta_0\n=\n\\bar u - (\\hat\\beta_1-\\beta_1)\\bar X.\n\\]Condition \\(X\\):zero conditional mean iterated expectations, \\(\\mathbb{E}[\\bar u\\mid X]=0\\).C2, \\(\\mathbb{E}[\\hat\\beta_1-\\beta_1\\mid X]=0\\).Thus:\\[\n\\mathbb{E}[\\hat\\beta_0-\\beta_0\\mid X]=0\n\\quad\\Rightarrow\\quad\n\\boxed{\\mathbb{E}[\\hat\\beta_0\\mid X]=\\beta_0.}\n\\]Unconditional unbiasedness follows iterated expectations .","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"c4.-why-uncorrelatedness-is-not-enough-concept-check","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.4 C4. Why uncorrelatedness is not enough (concept check)","text":"Narrative idea. Students often confuse unconditional statement “\\(\\operatorname{Cov}(X,u)=0\\)” conditional statement “\\(\\mathbb{E}[u\\mid X]=0\\)”. OLS unbiasedness needs conditional restriction.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-2-lines-1","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.4.1 Question (2 lines)","text":"Explain briefly \\(\\operatorname{Cov}(X,u)=0\\) alone guarantee OLS unbiasedness.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-10","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.4.2 Solution","text":"\\(\\operatorname{Cov}(X,u)=0\\) unconditional moment condition can hold even \\(\\mathbb{E}[u\\mid X]\\) varies \\(X\\) (e.g., nonlinear way). OLS unbiasedness relies conditional restriction \\(\\mathbb{E}[u\\mid X]=0\\), stronger rules systematic dependence mean error \\(X\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"quick-simulation-intuition-in-r-1","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.3.5 Quick simulation intuition in R","text":"chunk illustrates unbiasedness repeated samples \\(\\mathbb{E}[u\\mid X]=0\\). sample average \\(\\hat\\beta_1\\) across many simulations close true \\(\\beta_1\\).","code":"\nset.seed(123)\n\nB <- 2000\nn <- 200\nbeta0 <- 1\nbeta1 <- 2\n\nb1_hat <- numeric(B)\n\nfor (b in 1:B) {\n  X <- rnorm(n)\n  u <- rnorm(n)               # independent => E[u|X]=0\n  Y <- beta0 + beta1 * X + u\n  b1_hat[b] <- coef(lm(Y ~ X))[2]\n}\n\nc(\n  mean_b1_hat = mean(b1_hat),\n  true_beta1 = beta1\n)## mean_b1_hat  true_beta1 \n##    1.999375    2.000000"},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"part-d.-sampling-variance-of-ols-and-estimating-sigma2-with-solutions","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4 Part D. Sampling variance of OLS and estimating \\(\\sigma^2\\) (with solutions)","text":"Narrative idea. Part C showed OLS unbiased ..d. sampling zero conditional mean.\nPart D asks different question: variable OLS across samples? , variance \\(\\hat\\beta_1\\) \\(\\hat\\beta_0\\)?\nget clean formulas, add variance assumption (homoskedasticity) weak independence condition across observations.maintain model:\\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i,\n\\qquad \\mathbb{E}[u_i\\mid X_i]=0.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"d1.-assumptions-for-the-classical-variance-formulas","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.1 D1. Assumptions for the classical variance formulas","text":"derive simple variance expressions, assume:Homoskedasticity\n\\[\n\\operatorname{Var}(u_i\\mid X_i)=\\sigma^2 \\quad \\text{(constant } X_i\\text{)}.\n\\]Homoskedasticity\n\\[\n\\operatorname{Var}(u_i\\mid X_i)=\\sigma^2 \\quad \\text{(constant } X_i\\text{)}.\n\\]conditional correlation across observations\n\\[\n\\operatorname{Cov}(u_i,u_j\\mid X_1,\\dots,X_n)=0 \\quad (\\neq j).\n\\]conditional correlation across observations\n\\[\n\\operatorname{Cov}(u_i,u_j\\mid X_1,\\dots,X_n)=0 \\quad (\\neq j).\n\\]Define:\n\\[\nS_{xx} \\equiv \\sum_{=1}^n (X_i-\\bar X)^2.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"d2.-conditional-variance-of-the-slope","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.2 D2. Conditional variance of the slope","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-9","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.2.1 Question","text":"Using representation Part C,\\[\n\\hat\\beta_1-\\beta_1\n=\n\\frac{\\sum_{=1}^n (X_i-\\bar X)u_i}{S_{xx}},\n\\]show :\\[\n\\boxed{\\operatorname{Var}(\\hat\\beta_1\\mid X_1,\\dots,X_n)=\\frac{\\sigma^2}{S_{xx}}.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-11","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.2.2 Solution","text":"Condition full regressor sample \\(X=(X_1,\\dots,X_n)\\). \\(S_{xx}\\) \\((X_i-\\bar X)\\) constants. Compute:\\[\n\\operatorname{Var}(\\hat\\beta_1\\mid X)\n=\n\\operatorname{Var}\\left(\\frac{1}{S_{xx}}\\sum (X_i-\\bar X)u_i \\Bigm| X\\right)\n=\n\\frac{1}{S_{xx}^2}\\operatorname{Var}\\left(\\sum (X_i-\\bar X)u_i \\Bigm| X\\right).\n\\]Using conditional uncorrelatedness across \\(\\):\\[\n\\operatorname{Var}\\left(\\sum (X_i-\\bar X)u_i \\mid X\\right)\n=\n\\sum (X_i-\\bar X)^2\\operatorname{Var}(u_i\\mid X)\n=\n\\sum (X_i-\\bar X)^2\\sigma^2\n=\n\\sigma^2 S_{xx}.\n\\]Therefore:\\[\n\\operatorname{Var}(\\hat\\beta_1\\mid X)\n=\n\\frac{1}{S_{xx}^2}(\\sigma^2 S_{xx})\n=\n\\boxed{\\frac{\\sigma^2}{S_{xx}}.}\n\\]Interpretation. slope precise () noise smaller (\\(\\sigma^2\\) small) /(ii) \\(X\\) spread (\\(S_{xx}\\) large).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"d3.-conditional-variance-of-the-intercept","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.3 D3. Conditional variance of the intercept","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-10","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.3.1 Question","text":"Show :\\[\n\\boxed{\\operatorname{Var}(\\hat\\beta_0\\mid X)\n=\n\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}\\right).}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-12","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.3.2 Solution","text":"Use:\n\\[\n\\hat\\beta_0 = \\bar Y - \\hat\\beta_1\\bar X.\n\\]model, \\(\\bar Y = \\beta_0+\\beta_1\\bar X+\\bar u\\), :\\[\n\\hat\\beta_0-\\beta_0 = \\bar u - (\\hat\\beta_1-\\beta_1)\\bar X.\n\\]Condition \\(X\\). \\(\\bar X\\) constant, compute:\\[\n\\operatorname{Var}(\\hat\\beta_0\\mid X)\n=\n\\operatorname{Var}(\\bar u\\mid X)\n+ \\bar X^2\\operatorname{Var}(\\hat\\beta_1\\mid X)\n-2\\bar X\\operatorname{Cov}(\\bar u,\\hat\\beta_1\\mid X).\n\\]classical assumptions, \\(\\operatorname{Var}(\\bar u\\mid X)=\\sigma^2/n\\). Also already \\(\\operatorname{Var}(\\hat\\beta_1\\mid X)=\\sigma^2/S_{xx}\\).remains show covariance term zero. Using\n\\[\n\\hat\\beta_1-\\beta_1 = \\frac{1}{S_{xx}}\\sum (X_i-\\bar X)u_i,\n\\qquad\n\\bar u = \\frac{1}{n}\\sum u_i,\n\\]\ncovariance proportional :\n\\[\n\\operatorname{Cov}\\left(\\sum u_i,\\ \\sum (X_i-\\bar X)u_i \\mid X\\right)\n=\n\\sum (X_i-\\bar X)\\operatorname{Var}(u_i\\mid X)\n=\n\\sigma^2 \\sum (X_i-\\bar X)=0.\n\\]Hence:\n\\[\n\\operatorname{Var}(\\hat\\beta_0\\mid X)\n=\n\\frac{\\sigma^2}{n}+\\bar X^2\\frac{\\sigma^2}{S_{xx}}\n=\n\\boxed{\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}\\right).}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"d4.-estimating-sigma2-the-residual-variance-estimator","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.4 D4. Estimating \\(\\sigma^2\\): the residual variance estimator","text":"Define residuals:\\[\n\\hat u_i = Y_i-\\hat\\beta_0-\\hat\\beta_1X_i.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-11","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.4.1 Question","text":"State usual estimator \\(\\sigma^2\\) explain degrees freedom.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-13","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.4.2 Solution","text":"standard estimator :\\[\n\\boxed{\\hat\\sigma^2\n=\n\\frac{1}{n-2}\\sum_{=1}^n \\hat u_i^2.}\n\\]\\(n-2\\)? Two parameters \\((\\beta_0,\\beta_1)\\) estimated. residuals constrained two normal equations (Part ), remaining free variation used estimate \\(\\sigma^2\\) corresponds \\(n-2\\) degrees freedom.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"d5.-estimated-variance-and-standard-errors-of-ols","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.5 D5. Estimated variance and standard errors of OLS","text":"Plug \\(\\hat\\sigma^2\\):\\[\n\\boxed{\\widehat{\\operatorname{Var}}(\\hat\\beta_1\\mid X)=\\frac{\\hat\\sigma^2}{S_{xx}}}\n\\qquad\\Rightarrow\\qquad\n\\boxed{\\text{s.e.}(\\hat\\beta_1)=\\sqrt{\\frac{\\hat\\sigma^2}{S_{xx}}}}.\n\\]\\[\n\\boxed{\\widehat{\\operatorname{Var}}(\\hat\\beta_0\\mid X)=\\hat\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}\\right)}\n\\qquad\\Rightarrow\\qquad\n\\boxed{\\text{s.e.}(\\hat\\beta_0)=\\sqrt{\\hat\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}\\right)}}.\n\\]Interpretation. Standard errors translate sampling variability scale allows inference (confidence intervals t-tests).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"quick-check-in-r-using","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.4.6 Quick check in R using ","text":"","code":"\nset.seed(123)\n\nn <- 200\nX <- rnorm(n, mean = 2, sd = 1.5)\nu <- rnorm(n, mean = 0, sd = 2)\nY <- 1 + 1.5*X + u\n\nfit <- lm(Y ~ X)\n\n# Manual pieces for the classical formulas\nuhat <- resid(fit)\nsigma2_hat <- sum(uhat^2)/(n-2)\nSxx <- sum((X - mean(X))^2)\n\nse_b1_manual <- sqrt(sigma2_hat / Sxx)\nse_b0_manual <- sqrt(sigma2_hat * (1/n + mean(X)^2 / Sxx))\n\nc(\n  se_b0_lm = summary(fit)$coef[1,2],\n  se_b0_manual = se_b0_manual,\n  se_b1_lm = summary(fit)$coef[2,2],\n  se_b1_manual = se_b1_manual\n)##     se_b0_lm se_b0_manual     se_b1_lm se_b1_manual \n##    0.2437701    0.2437701    0.1000182    0.1000182"},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"part-e.-functional-form-and-units-interpreting-coefficients-correctly-with-solutions","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5 Part E. Functional form and units: interpreting coefficients correctly (with solutions)","text":"Narrative idea. Even OLS unbiased know variance, still need interpret coefficients correctly.\ncoefficient number units, functional form choose (levels vs logs, scaling) determines meaning “one unit increase.”use one running example:\\(Y\\) = weekly earnings (dollars)\\(X\\) = hours worked per week","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e1.-units-of-the-slope-in-the-levellevel-model","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.1 E1. Units of the slope in the level–level model","text":"Consider:\\[\nY = \\beta_0 + \\beta_1 X + u.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-12","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.1.1 Question","text":"units \\(\\beta_1\\)?Give economic interpretation \\(\\beta_1\\) words.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-14","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.1.2 Solution","text":"\\(Y\\) dollars \\(X\\) hours, \\(\\beta_1\\) units dollars per hour.\\(Y\\) dollars \\(X\\) hours, \\(\\beta_1\\) units dollars per hour.\\(\\beta_1\\) change expected weekly earnings associated one additional hour worked per week, holding unobservables \\(u\\) fixed conditional-mean sense (\\(\\mathbb{E}[u\\mid X]=0\\)).\\(\\beta_1\\) change expected weekly earnings associated one additional hour worked per week, holding unobservables \\(u\\) fixed conditional-mean sense (\\(\\mathbb{E}[u\\mid X]=0\\)).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e2.-rescaling-regressors-why-coefficients-change-mechanically","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.2 E2. Rescaling regressors: why coefficients change mechanically","text":"Define rescaled regressor:\\[\nX^{(10)} \\equiv \\frac{X}{10}.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-13","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.2.1 Question","text":"regress \\(Y\\) \\(X^{(10)}\\), slope change? Relate \\(\\beta_1^{(10)}\\) \\(\\beta_1\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-15","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.2.2 Solution","text":"Since \\(X = 10X^{(10)}\\), substitute original model:\\[\nY = \\beta_0 + \\beta_1(10X^{(10)}) + u\n= \\beta_0 + (10\\beta_1)X^{(10)} + u.\n\\]:\\[\n\\boxed{\\beta_1^{(10)} = 10\\beta_1.}\n\\]Interpretation. one-unit increase \\(X^{(10)}\\) 10-hour increase \\(X\\), slope scales accordingly.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e3.-rescaling-outcomes-what-changes-and-what-does-not","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.3 E3. Rescaling outcomes: what changes and what does not","text":"Define \\(Y^{(1000)} \\equiv Y/1000\\) (earnings “thousands dollars”).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-14","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.3.1 Question","text":"regress \\(Y^{(1000)}\\) \\(X\\), happens slope intercept?","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-16","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.3.2 Solution","text":"Divide entire equation 1000:\\[\n\\frac{Y}{1000} = \\frac{\\beta_0}{1000} + \\frac{\\beta_1}{1000}X + \\frac{u}{1000}.\n\\]Thus:\\[\n\\boxed{\\beta_0^{(1000)} = \\beta_0/1000, \\quad \\beta_1^{(1000)} = \\beta_1/1000.}\n\\]Interpretation. Changing units dependent variable rescales coefficients, change underlying relationship—measurement scale.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e4.-loglevel-model-interpreting-semi-elasticities","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.4 E4. Log–level model: interpreting semi-elasticities","text":"Consider:\\[\n\\ln(Y) = \\gamma_0 + \\gamma_1 X + v.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-15","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.4.1 Question","text":"Interpret \\(\\gamma_1\\). Give rule--thumb interpretation small \\(\\gamma_1\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-17","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.4.2 Solution","text":"\\(\\gamma_1\\) semi-elasticity: measures change log earnings one-unit increase \\(X\\).small \\(\\gamma_1\\):\\[\n\\Delta \\ln(Y) \\approx \\frac{\\Delta Y}{Y}.\n\\], approximately:\\[\n\\boxed{\\text{1-unit increase }X\\text{ associated }100\\gamma_1\\%\\text{ change }Y.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e5.-loglog-model-interpreting-elasticities","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.5 E5. Log–log model: interpreting elasticities","text":"Consider:\\[\n\\ln(Y) = \\delta_0 + \\delta_1 \\ln(X) + e.\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-16","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.5.1 Question","text":"Interpret \\(\\delta_1\\).","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-18","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.5.2 Solution","text":"\\(\\delta_1\\) elasticity:\\[\n\\boxed{\\text{1\\% increase }X\\text{ associated }\\delta_1\\%\\text{ change }Y.}\n\\]","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"e6.-functional-form-as-a-modeling-choice-concept-check","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.6 E6. Functional form as a modeling choice (concept check)","text":"","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"question-short","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.6.1 Question (short)","text":"Give one reason prefer logs (log–level log–log) rather levels.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"solution-19","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.6.2 Solution","text":"Logs often preferred variation \\(Y\\) roughly proportional level (e.g., earnings), can make relationships closer linear logs can reduce heteroskedasticity. Logs also lead percent-change interpretations often meaningful “dollar changes” across different income levels.","code":""},{"path":"tutorial-3-simple-ols-residuals-assumptions-unbiasedness-variance-and-interpretation.html","id":"small-r-demo-same-data-different-scales","chapter":"4 Tutorial 3: Simple OLS — Residuals, Assumptions, Unbiasedness, Variance, and Interpretation","heading":"4.5.7 Small R demo: same data, different scales","text":"see approximately:\n- b1_rescaleX ≈ 10 * b1_level\n- b1_rescaleY ≈ b1_level / 1000","code":"\nset.seed(123)\n\nn <- 200\nX <- rnorm(n, mean = 40, sd = 5)        # hours per week\nu <- rnorm(n, mean = 0, sd = 50)\nY <- 200 + 15*X + u                     # dollars per week\n\nfit_level <- lm(Y ~ X)\n\nX10 <- X/10\nfit_rescaleX <- lm(Y ~ X10)\n\nYk <- Y/1000\nfit_rescaleY <- lm(Yk ~ X)\n\nc(\n  b1_level = coef(fit_level)[2],\n  b1_rescaleX = coef(fit_rescaleX)[2],\n  b1_rescaleY = coef(fit_rescaleY)[2]\n)##      b1_level.X b1_rescaleX.X10   b1_rescaleY.X \n##     14.70745558    147.07455583      0.01470746"},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","text":"Duration: 50 minutes | Based Wooldridge, Sections 1-4 2-7a","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"block-1-motivation-why-do-we-need-potential-outcomes-5-min","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.1 Block 1: Motivation — Why Do We Need Potential Outcomes? (5 min)","text":"Causality, Ceteris Paribus, Counterfactual Reasoning (Section 1-4)economics want know whether one variable causal effect another. requires ceteris paribus reasoning: happens outcome change one factor, holding factors fixed. Since can rarely hold factors fixed, resort counterfactual reasoning: “happened different state world?” challenge can never observe unit states simultaneously. fundamental problem causal inference. textbook illustrates four examples:Ex. 1.3 — Fertilizer Crop Yield. farmer obtains 180 bushels/acre using fertilizer. counterfactual (yield without fertilizer plot, season) unobservable. ideal experiment randomly assigns fertilizer identical plots. practice, farmers choose fertilizer based soil quality — confounding factor also affects yield.Ex. 1.3 — Fertilizer Crop Yield. farmer obtains 180 bushels/acre using fertilizer. counterfactual (yield without fertilizer plot, season) unobservable. ideal experiment randomly assigns fertilizer identical plots. practice, farmers choose fertilizer based soil quality — confounding factor also affects yield.Ex. 1.4 — Return Education. person gets one year education, much wage rise? social planner randomly assign education levels, infeasible. People choose education based ability background, comparing wages across education levels confounds education effect pre-existing differences (selection bias).Ex. 1.4 — Return Education. person gets one year education, much wage rise? social planner randomly assign education levels, infeasible. People choose education based ability background, comparing wages across education levels confounds education effect pre-existing differences (selection bias).Ex. 1.5 — Police Crime. hiring police reduce crime? Cities high crime already hire police, creating positive correlation even police reduce crime. reverse causality (simultaneity).Ex. 1.5 — Police Crime. hiring police reduce crime? Cities high crime already hire police, creating positive correlation even police reduce crime. reverse causality (simultaneity).Ex. 1.6 — Minimum Wage Unemployment. Political economic forces set minimum wage also affect employment. impossible isolate causal effect without holding confounding forces fixed.Ex. 1.6 — Minimum Wage Unemployment. Political economic forces set minimum wage also affect employment. impossible isolate causal effect without holding confounding forces fixed.case, core problem : observe one state world need compare counterfactual see. potential outcomes framework (Section 2-7a) formalizes problem mathematically, random assignment provides cleanest solution.Question 1 (Quick warm-)() Example 1.4, can simply compare average wage people 16 years education people 15 years call difference causal return education?Solution. two groups differ many ways (ability, family background, motivation, etc.). naive comparison mixes effect education pre-existing differences. hold “factors fixed” just comparing different people. selection bias: people choose education systematically different choose less.(b) Examples 1.3–1.6, describes ideal experiment infeasible. one feature ideal experiments share solve causal inference problem?Solution. Random assignment treatment. every case, ideal experiment randomly assigns treatment (fertilizer amounts, education levels, police force sizes, minimum wage levels) treatment status independent characteristics. eliminates confounding selection bias, making simple group comparisons valid estimates causal effects.","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"block-2-potential-outcomes-notation-13-min","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.2 Block 2: Potential Outcomes Notation (13 min)","text":"Section 2-7a — Counterfactual (Potential) Outcomes, Causality, Policy AnalysisConsider binary treatment: \\(x_i = 1\\) unit \\(\\) treatment group, \\(x_i = 0\\) control group. unit \\(\\) two potential outcomes: \\(y_i(1)\\) (outcome treated) \\(y_i(0)\\) (outcome treated). exist conceptually, observe one.individual treatment effect \\(\\tau_i = y_i(1) - y_i(0)\\).Average Treatment Effect (ATE) Average Treatment Effect Treated (ATT) :\\[ATE = E[y(1)] - E[y(0)] \\tag{2.75}\\]\\[ATT = E[y(1) - y(0) \\mid x = 1] \\tag{2.76}\\]observed outcome given “switching equation”:\\[y_i = x_i \\cdot y_i(1) + (1 - x_i) \\cdot y_i(0) \\tag{2.77}\\]Given random sample, observe one \\(y_i(0)\\) \\(y_i(1)\\): treatment “switches” potential outcome see.Question 2 (Notation — Definitions)Consider binary treatment \\(x_i \\\\{0,1\\}\\) individual \\(\\).() Define two potential outcomes \\(y_i(0)\\) \\(y_i(1)\\). represent?Solution. \\(y_i(1)\\) = outcome individual \\(\\) experience treated (\\(x_i = 1\\)).\\(y_i(0)\\) = outcome individual \\(\\) experience treated (\\(x_i = 0\\)).defined every individual regardless actual treatment status. person, potential outcomes exist conceptually, can observe one .(b) Define individual treatment effect \\(\\tau_i\\). can never compute directly?Solution. \\[\\tau_i = y_i(1) - y_i(0)\\]can never compute observe one \\(y_i(1)\\) \\(y_i(0)\\), never . fundamental problem causal inference restated potential outcomes notation.(c) Write observed outcome \\(y_i\\) terms \\(x_i\\), \\(y_i(0)\\), \\(y_i(1)\\) (equation [2.77]). Verify formula value \\(x_i\\).Solution. \\[y_i = x_i \\cdot y_i(1) + (1 - x_i)\\cdot y_i(0)\\]Verification:\\(x_i = 1\\): \\(y_i = 1 \\cdot y_i(1) + 0 \\cdot y_i(0) = y_i(1)\\) ✓\\(x_i = 0\\): \\(y_i = 0 \\cdot y_i(1) + 1 \\cdot y_i(0) = y_i(0)\\) ✓switching equation: treatment “switches” potential outcome observe.Question 3 (Math — Algebraic manipulation)Starting \\(y_i = x_i \\cdot y_i(1) + (1 - x_i)\\cdot y_i(0)\\):() Show equivalent :\\[y_i = y_i(0) + \\bigl[y_i(1) - y_i(0)\\bigr]\\cdot x_i = y_i(0) + \\tau_i \\cdot x_i\\]Solution. Start :\\[\ny_i = x_i \\cdot y_i(1) + (1 - x_i)\\cdot y_i(0)\n= x_i \\cdot y_i(1) + y_i(0) - x_i \\cdot y_i(0)\n= y_i(0) + x_i\\bigl[y_i(1) - y_i(0)\\bigr]\n\\]\\[\\boxed{y_i = y_i(0) + \\tau_i \\cdot x_i}\\]\\(\\tau_i = y_i(1) - y_i(0)\\). equation [2.78] book.(b) Now suppose treatment effect constant: \\(\\tau_i = \\tau\\) \\(\\). Let \\(\\beta_0 = E[y(0)]\\) \\(u_i = y_i(0) - E[y(0)]\\). Show :\\[y_i = \\beta_0 + \\tau \\cdot x_i + u_i\\]equation look like? connection regression?Solution. part (), constant \\(\\tau\\): \\(y_i = y_i(0) + \\tau \\cdot x_i\\).Write \\(y_i(0) = E[y(0)] + [y_i(0) - E[y(0)]] = \\beta_0 + u_i\\):\\[y_i = \\beta_0 + \\tau \\cdot x_i + u_i\\]simple linear regression model \\(y = \\beta_0 + \\beta_1 x + u\\), slope \\(\\beta_1 = \\tau\\) causal treatment effect (equation [2.79]). potential outcomes framework provides structural foundation regression model.(c) Write formulas ATE (eq. [2.75]) ATT (eq. [2.76]). one sentence , explain population one refers .Solution. \\[ATE = E[y(1) - y(0)] = E[y(1)] - E[y(0)]\\]ATE average effect across entire population (treated untreated).\\[ATT = E[y(1) - y(0) \\mid x = 1]\\]ATT average effect among actually received treatment .treatment effects heterogeneous correlated treatment selection, \\(ATE \\neq ATT\\).","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"block-3-ate-vs.-att-and-selection-bias-numerical-exercise-13-min","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.3 Block 3: ATE vs. ATT and Selection Bias — Numerical Exercise (13 min)","text":"Question 4 (Numerical exercise)Suppose population two types individuals:() Compute individual treatment effect \\(\\tau\\) type.Solution. Type : \\(\\tau_A = 5 - 2 = 3\\)Type B: \\(\\tau_B = 6 - 4 = 2\\)(b) Compute ATE.Solution. \\[ATE = E[\\tau] = 0.6 \\times 3 + 0.4 \\times 2 = 1.8 + 0.8 = \\boxed{2.6}\\](c) Now suppose Type individuals get treated (\\(x = 1\\) Type , \\(x = 0\\) Type B). Compute ATT. equal ATE? ?Solution. \\(ATT = E[\\tau \\mid x = 1] = \\tau_A = \\boxed{3}\\).\\(ATT = 3 \\neq 2.6 = ATE\\) Type larger treatment effect Type B. Treatment assignment correlated individual treatment effect — heterogeneous effects combined selection treatment.(d) Compute selection bias \\(E[y(0) \\mid x=1] - E[y(0) \\mid x=0]\\). sign? Interpret.Solution. \\[\\text{Selection Bias} = E[y(0) \\mid x=1] - E[y(0) \\mid x=0] = y_A(0) - y_B(0) = 2 - 4 = \\boxed{-2}\\]bias negative: treated individuals (Type ) lower baseline outcomes. Intuitively, worse without treatment, perhaps received .(e) Verify decomposition: compute sides \\[E[y \\mid x=1] - E[y \\mid x=0] = ATT + \\text{Selection Bias}\\]Solution. Left side: \\(E[y \\mid x=1] - E[y \\mid x=0] = y_A(1) - y_B(0) = 5 - 4 = 1\\).Right side: \\(ATT + \\text{Sel. Bias} = 3 + (-2) = 1\\). ✓naive comparison (\\(\\bar{y}_1 - \\bar{y}_0 = 1\\)) understates true ATT 3 negative selection bias.(f) Suppose instead treatment randomly assigned (individual gets \\(x=1\\) probability 0.5, regardless type). \\(E[\\bar{y}_1 - \\bar{y}_0]\\) estimate?Solution. random assignment, \\(x \\perp (y(0), y(1))\\), :Selection bias \\(= E[y(0) \\mid x=1] - E[y(0) \\mid x=0] = 0\\)\\(ATT = ATE = 2.6\\)Therefore \\(E[\\bar{y}_1 - \\bar{y}_0] = ATE = 2.6\\). simple difference means unbiased ATE.","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"block-4-the-selection-bias-decomposition-and-random-assignment-13-min","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.4 Block 4: The Selection Bias Decomposition and Random Assignment (13 min)","text":"Question 5 (Key derivation)now prove result used numerically Block 3. Let \\(\\bar{y}_1\\) average outcome among treated units \\(\\bar{y}_0\\) among untreated.() Show population version comparison \\(\\bar{y}_1 - \\bar{y}_0\\) decomposes :\\[\nE[y \\mid x=1] - E[y \\mid x=0]\n= \\underbrace{E\\bigl[y(1) - y(0) \\mid x=1\\bigr]}_{ATT}\n+ \\underbrace{E\\bigl[y(0) \\mid x=1\\bigr] - E\\bigl[y(0) \\mid x=0\\bigr]}_{\\text{Selection Bias}}\n\\]Hint: use switching equation write \\(E[y \\mid x=1] = E[y(1) \\mid x=1]\\) \\(E[y \\mid x=0] = E[y(0) \\mid x=0]\\). add subtract \\(E[y(0) \\mid x=1]\\).Solution. switching equation [2.77]:\\(x = 1\\): \\(y = y(1)\\), \\(E[y \\mid x=1] = E[y(1) \\mid x=1]\\).\\(x = 0\\): \\(y = y(0)\\), \\(E[y \\mid x=0] = E[y(0) \\mid x=0]\\).Therefore:\\[E[y \\mid x=1] - E[y \\mid x=0] = E[y(1) \\mid x=1] - E[y(0) \\mid x=0]\\]Add subtract \\(E[y(0) \\mid x=1]\\):\\[\n\\begin{align*}\n&= E[y(1) \\mid x=1] - E[y(0) \\mid x=1] + E[y(0) \\mid x=1] - E[y(0) \\mid x=0] \\\\[4pt]\n&= \\underbrace{E[y(1) - y(0) \\mid x=1]}_{ATT}\n+ \\underbrace{E[y(0) \\mid x=1] - E[y(0) \\mid x=0]}_{\\text{Selection Bias}} \\quad\\square\n\\end{align*}\n\\](b) Explain words \\(E[y(0) \\mid x=1] - E[y(0) \\mid x=0]\\) means. Use education example (Ex. 1.4) job training context give one example positive one negative selection bias.Solution. compares baseline outcome (without treatment) received treatment . treated individuals different outcomes even without treatment, naive comparison confounds pre-existing differences treatment effect.Positive bias (Education): able people self-select college. earn even without extra schooling, \\(E[y(0) \\mid x=1] > E[y(0) \\mid x=0]\\). naive wage gap overstates return education.Negative bias (Job training): Workers worse prospects sign training. earn less even without program, \\(E[y(0) \\mid x=1] < E[y(0) \\mid x=0]\\). naive comparison understates program’s effect.(c) Now suppose treatment randomly assigned. math, let us understand randomization works.random assignment “work”? — mathematical mechanismStep 0: key property independence. Recall \\(X \\perp Y\\), \\[E[Y \\mid X = x] = E[Y] \\quad \\text{} x\\]Conditioning independent variable nothing: conditional expectation equals unconditional one. single fact entire engine.Step 1: randomization create independence? experiment, unit \\(\\) already fixed potential outcomes \\((y_i(0), y_i(1))\\), determined person’s characteristics (ability, motivation, health, …). exist regardless .randomly assign treatment, \\(x_i\\) determined coin flip knows nothing unit \\(\\). Formally:\\[P(x_i = 1 \\mid y_i(0),\\, y_i(1)) = P(x_i = 1) = p \\quad \\text{} \\]person \\(y_i(0) = 100\\) probability \\(p\\) treated person \\(y_i(0) = 2\\). coin look potential outcomes. Therefore, construction:\\[\\boxed{x \\perp (y(0),\\; y(1))}\\]Step 2: Apply independence property. Since \\(x \\perp y(0)\\):\\[E[y(0) \\mid x=1] = E[y(0)] \\quad\\text{}\\quad E[y(0) \\mid x=0] = E[y(0)]\\]treated group control group average baseline — identical person--person, sorting mechanism (coin) unrelated individual characteristics. Likewise, \\(x \\perp y(1)\\) gives \\(E[y(1) \\mid x=1] = E[y(1)]\\).Step 3: fail without randomization? Without randomization, people choose (selected ) treatment based characteristics — characteristics determine \\((y_i(0), y_i(1))\\). \\(P(x_i = 1 \\mid y_i(0), y_i(1)) \\neq P(x_i = 1)\\): probability treatment depends potential outcomes. example, high-ability people likely go college, \\(E[y(0) \\mid x=1] > E[y(0) \\mid x=0]\\). Conditioning \\(x = 1\\) selects non-representative subgroup, independence property fails.Now use property \\(x \\perp (y(0), y(1))\\) show three things:() Selection bias \\(= 0\\).(ii) \\(ATT = ATE\\).(iii) \\(\\bar{y}_1 - \\bar{y}_0\\) unbiased ATE.Solution. note , independence gives us:\\[E[y(0) \\mid x=1] = E[y(0) \\mid x=0] = E[y(0)]\n\\quad\\text{}\\quad\nE[y(1) \\mid x=1] = E[y(1) \\mid x=0] = E[y(1)]\\]() Selection bias vanishes:\\[E[y(0) \\mid x=1] - E[y(0) \\mid x=0] = E[y(0)] - E[y(0)] = 0\\](ii) ATT = ATE:\\[ATT = E[y(1) - y(0) \\mid x=1] = E[y(1)] - E[y(0)] = ATE\\](iii) Unbiasedness: now prove \\(E[\\bar{y}_1 - \\bar{y}_0] = ATE\\) step step.Definition. estimator \\(\\hat{\\theta}\\) unbiased \\(\\theta\\) \\(E[\\hat{\\theta}] = \\theta\\). \\(\\hat{\\theta} = \\bar{y}_1 - \\bar{y}_0\\) \\(\\theta = ATE = E[y(1) - y(0)]\\).Step 1 — observe group. Treated individuals reveal \\(y(1)\\); control individuals reveal \\(y(0)\\):\\[E[\\bar{y}_1] = E[y \\mid x=1] = E[y(1) \\mid x=1]\\]\\[E[\\bar{y}_0] = E[y \\mid x=0] = E[y(0) \\mid x=0]\\]Step 2 — Apply random assignment. Since \\(x \\perp (y(0), y(1))\\), conditioning \\(x\\) nothing:\\[E[y(1) \\mid x=1] = E[y(1)], \\qquad E[y(0) \\mid x=0] = E[y(0)]\\]Step 3 — Substitute conclude.\\[\n\\begin{align*}\n  E[\\bar{y}_1 - \\bar{y}_0]\n    &= E[y(1) \\mid x=1] - E[y(0) \\mid x=0] & \\text{(Step 1)}\\\\\n    &= E[y(1)] - E[y(0)]                    & \\text{(Step 2)}\\\\\n    &= E[\\,y(1) - y(0)\\,]                   & \\text{(linearity expectation)}\\\\\n    &= ATE                                   & \\checkmark\n\\end{align*}\n\\]Intuition. fundamental problem causal inference never observe \\(y(1)\\) \\(y(0)\\) person. \\(x \\perp (y(0), y(1))\\), treated group random sample population — average \\(y(1)\\) represents everyone’s \\(y(1)\\) — likewise control group’s average \\(y(0)\\) represents everyone’s \\(y(0)\\). group serves valid counterfactual , selection bias cancels exactly. \\(\\square\\)RCTs gold standard. random assignment, OLS gives \\(\\hat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0\\), unbiased ATE (equation [2.82]).(d) (Problem 15 book) sample average treatment effect estimator \\(\\hat{\\tau}_{ate} = n^{-1}\\sum_{=1}^{n}[y_i(1) - y_i(0)]\\). Show observe potential outcomes everyone, \\(\\hat{\\tau}_{ate}\\) unbiased \\(\\tau_{ate} = E[y(1) - y(0)]\\).Solution. \\[\nE[\\hat{\\tau}_{ate}]\n= E\\!\\left[n^{-1}\\sum_{=1}^{n}[y_i(1) - y_i(0)]\\right]\n= n^{-1}\\sum_{=1}^{n} E[y_i(1) - y_i(0)]\n\\]Since sample ..d.:\\[= n^{-1} \\cdot n \\cdot E[y(1) - y(0)] = \\tau_{ate}\\]\\(\\hat{\\tau}_{ate}\\) unbiased. course, can never compute don’t observe \\(y_i(0)\\) \\(y_i(1)\\). random assignment, \\(\\bar{y}_1 - \\bar{y}_0\\) takes place unbiased estimator. \\(\\square\\)(e) Explain briefly \\(\\bar{y}_1\\) \\(\\bar{y}(1) \\equiv n^{-1}\\sum_{=1}^{n} y_i(1)\\) thing.Solution. \\(\\bar{y}(1)\\) averages \\(y_i(1)\\) \\(n\\) individuals (treated untreated). \\(\\bar{y}_1\\) averages observed outcomes \\(n_1\\) treated individuals.\\(\\bar{y}(1)\\) requires knowing \\(y_i(1)\\) untreated people — never observe. random assignment, \\(\\bar{y}_1\\) unbiased \\(E[y(1)]\\), serves valid substitute.","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"block-5-application-job-training-program-6-min","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.5 Block 5: Application — Job Training Program (6 min)","text":"Example 2.14 — Evaluating Job Training Program (JTRAIN2)data JTRAIN2 National Supported Work demonstration, men randomly assigned receive job training (9–18 months) serve controls. outcome re78: real earnings 1978 (thousands 1982 dollars). treatment variable train (\\(= 1\\) trained).445 men, 185 received training 260 . simple regression \\(\\widehat{re78} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot train\\) gives: \\(\\hat{\\beta}_0 = 4.555\\) (control group average, thousands), \\(\\hat{\\beta}_1 = 1.794\\) (treated earned $1,794 average). \\(t\\)-statistic 1.79, \\(R^2 \\approx 0.018\\).small \\(R^2\\) means training explains 1.8% earnings variation. \\(R^2\\) measures explanatory power, causal significance — random assignment, \\(\\hat{\\beta}_1\\) still unbiased ATE.Question 6 (Example 2.14 book)() Express \\(\\bar{y}_1\\), \\(\\bar{y}_0\\), \\(\\hat{\\beta}_1\\) terms . Compute \\(\\bar{y}_1\\).Solution. \\(\\bar{y}_0 = \\hat{\\beta}_0 = 4.555\\) (control mean). \\(\\bar{y}_1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 = 4.555 + 1.794 = 6.349\\) (treated mean).\\[\\hat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0 = 1.794\\]equation [2.82]: OLS slope binary regressor equals difference group means.(b) Can interpret \\(\\hat{\\beta}_1 = 1.794\\) causally? ? Express effect percentage control mean.Solution. Yes, training randomly assigned: \\(E[y(0) \\mid x=1] = E[y(0) \\mid x=0]\\), selection bias \\(= 0\\) \\(\\hat{\\beta}_1\\) unbiased ATE.percentage: \\(1.794 / 4.555 \\approx 39.4\\%\\) increase earnings.(c) \\(R^2\\) 0.018. student says: “training effect \\(R^2\\) tiny.” correct?Solution. . \\(R^2\\) measures much total variation earnings explained training dummy alone. Since many factors affect earnings (ability, experience, education, luck), small \\(R^2\\) expected. says nothing whether treatment effect real economically meaningful. $1,794 effect (39% control earnings) substantial; \\(t\\)-stat 1.79 indicates borderline statistical significance.(d) workers volunteered instead randomly assigned, still trust estimate? ?Solution. . self-selection, \\(E[y(0) \\mid x=1] \\neq E[y(0) \\mid x=0]\\). Motivated workers may volunteer (positive selection bias \\(\\Rightarrow\\) overestimate), workers poor prospects may seek help (negative selection bias \\(\\Rightarrow\\) underestimate). difference means captures treatment effect plus selection bias, separate .","code":""},{"path":"tutorial-5-potential-outcomes-causality-and-counterfactual-reasoning.html","id":"summary-of-key-formulas","chapter":"5 Tutorial 5: Potential Outcomes, Causality, and Counterfactual Reasoning","heading":"5.6 Summary of Key Formulas","text":"","code":""}]
